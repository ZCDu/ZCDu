<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>pytorch | 赵某人の杂物室</title><meta name="keywords" content="Pytorch"><meta name="author" content="ZCDu"><meta name="copyright" content="ZCDu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="模型ConvTranspose2did:: 6258d8b8-2557-4a01-ae49-cb96116dd746逆卷积。论文中称其为fractionally-strided convolutions，也有的称它为deconvolutions，但是前者表达更为确切。 nn.AdaptiveAvgPool2d2元(2d)就是二维数据的意思。 汇聚层（Pooling）汇聚层，有些地方也翻译成池化层，">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch">
<meta property="og:url" content="http://example.com/2022/04/15/pytorch/index.html">
<meta property="og:site_name" content="赵某人の杂物室">
<meta property="og:description" content="模型ConvTranspose2did:: 6258d8b8-2557-4a01-ae49-cb96116dd746逆卷积。论文中称其为fractionally-strided convolutions，也有的称它为deconvolutions，但是前者表达更为确切。 nn.AdaptiveAvgPool2d2元(2d)就是二维数据的意思。 汇聚层（Pooling）汇聚层，有些地方也翻译成池化层，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img2.baidu.com/it/u=3234735447,1138674362&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=471">
<meta property="article:published_time" content="2022-04-15T14:21:39.000Z">
<meta property="article:modified_time" content="2023-01-21T11:59:37.594Z">
<meta property="article:author" content="ZCDu">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img2.baidu.com/it/u=3234735447,1138674362&fm=253&fmt=auto&app=138&f=JPEG?w=500&h=471"><link rel="shortcut icon" href="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/favicon.png"><link rel="canonical" href="http://example.com/2022/04/15/pytorch/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-01-21 11:59:37'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.1.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">36</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comment"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 镜像</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://zcdu.gitee.io/"><i class="fa-fw fab fa-gitlab"></i><span> Gitee</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://ZCDu.github.io/"><i class="fa-fw fab fa-github"></i><span> Github</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://zcdu-8lbl7zwf-zcdu.4everland.app/"><i class="fa-fw fab fa-github"></i><span> 4everland1</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://zcdu.4everland.app/"><i class="fa-fw fab fa-github"></i><span> 4everland2</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情出演</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img2.baidu.com/it/u=3234735447,1138674362&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=500&amp;h=471')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">赵某人の杂物室</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comment"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 镜像</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://zcdu.gitee.io/"><i class="fa-fw fab fa-gitlab"></i><span> Gitee</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://ZCDu.github.io/"><i class="fa-fw fab fa-github"></i><span> Github</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://zcdu-8lbl7zwf-zcdu.4everland.app/"><i class="fa-fw fab fa-github"></i><span> 4everland1</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://zcdu.4everland.app/"><i class="fa-fw fab fa-github"></i><span> 4everland2</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情出演</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-15T14:21:39.000Z" title="发表于 2022-04-15 14:21:39">2022-04-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-01-21T11:59:37.594Z" title="更新于 2023-01-21 11:59:37">2023-01-21</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">16.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>64分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="pytorch"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><h2 id="ConvTranspose2d"><a href="#ConvTranspose2d" class="headerlink" title="ConvTranspose2d"></a>ConvTranspose2d</h2><p>id:: 6258d8b8-2557-4a01-ae49-cb96116dd746<br>逆卷积。<br>论文中称其为fractionally-strided convolutions，也有的称它为deconvolutions，但是前者表达更为确切。</p>
<h2 id="nn-AdaptiveAvgPool2d"><a href="#nn-AdaptiveAvgPool2d" class="headerlink" title="nn.AdaptiveAvgPool2d"></a>nn.AdaptiveAvgPool2d</h2><h3 id="2元-2d"><a href="#2元-2d" class="headerlink" title="2元(2d)"></a>2元(2d)</h3><p>就是二维数据的意思。</p>
<h3 id="汇聚层（Pooling）"><a href="#汇聚层（Pooling）" class="headerlink" title="汇聚层（Pooling）"></a>汇聚层（Pooling）</h3><p>汇聚层，有些地方也翻译成池化层，它主要负责对数据在空间维度（宽度和高度）上进行降采样（downsampling）操作</p>
<h3 id="均值（Avg）"><a href="#均值（Avg）" class="headerlink" title="均值（Avg）"></a>均值（Avg）</h3><p>均值（Avg）指定了汇聚层在进行降采样操作时所采用的计算方法。<br>汇聚层在降采样时，通常会使用<strong>最大值抽取样本和均值抽取样本</strong>两种手法。用最大值抽取样本的汇聚层一般叫做最大值汇聚层，用均值抽取样本的汇聚层一般叫做均值汇聚层。</p>
<h3 id="自适应-Adaptive"><a href="#自适应-Adaptive" class="headerlink" title="自适应(Adaptive)"></a>自适应(Adaptive)</h3><p>在实际的项目当中，我们往往预先只知道的是输入数据和输出数据的大小，而不知道核与步长的大小。如果使用上面的方法创建汇聚层，我们每次都需要手动计算核的大小和步长的值。而自适应（Adaptive）能让我们从这样的计算当中解脱出来，只要我们给定输入数据和输出数据的大小，自适应算法能够自动帮助我们计算核的大小和每次移动的步长。相当于我们对核说，我已经给你输入和输出的数据了，你自己适应去吧。你要长多大，你每次要走多远，都由你自己决定，总之最后你的输出符合我的要求就行了。</p>
<h3 id="nn-AvgPool2d"><a href="#nn-AvgPool2d" class="headerlink" title="nn.AvgPool2d"></a>nn.AvgPool2d</h3><p>需要至少指定kernel_size,stride，padding 三个参数，使用起来和卷积相似。最后的输出为<br><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649989945052_0.png" alt="image.png"><br><strong>AvgPool2d和AdaptiveAvgPool2d</strong>: 前者使用方法同卷积操作，而后者只需要指定输出图像的尺度，如HxW,或H（这时会默认为HxH）。</p>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=<span class="literal">None</span>, max_norm=<span class="literal">None</span>, norm_type=<span class="number">2</span>, scale_grad_by_freq=<span class="literal">False</span>, sparse=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>num_embeddings：字典中词的个数<br>embedding_dim:嵌入的维度<br>padding_idx （索引指定填充）：如果给定，则遇到padding_idx中的索引，则将其位置填0（0是默认值，事实上随便填充什么值都可以）<br>max_norm:每一个嵌入矢量，它的norm如果大于max_norm将会被renormalized到max_norm<br>norm_type: the p of the p-norm to compute for the max_norm option.Default 2<br>scale_grad_by_freq: this will scale gradients by the inverse of frequence of the words in the min-batch .Default False<br>sparse:是否使用稀疏矩阵</p>
<blockquote>
<p>这个函数的作用就是词嵌入，避免了手工的词袋设计。</p>
</blockquote>
<p>🔥 embeddings中的值是正太分布N(0,1)中随机取值。</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h4 id="传统的使用方法"><a href="#传统的使用方法" class="headerlink" title="传统的使用方法"></a>传统的使用方法</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649990056626_0.png" alt="image.png"></p>
<h4 id="在Sparse-R-CNN中用来嵌入框"><a href="#在Sparse-R-CNN中用来嵌入框" class="headerlink" title="在Sparse R-CNN中用来嵌入框"></a>在Sparse R-CNN中用来嵌入框</h4><p>将框作为嵌入信息使用，使用embeddings层的权重作为可能的位置关系来进行，通过clone权重作为参数，将字典功能的嵌入层变为了特征转换层。Sparse R-CNN&#x2F;projects&#x2F;SparseRCNN&#x2F;sparsercnn&#x2F;detector.py&#x2F;131</p>
<h2 id="传统的使用方法-1"><a href="#传统的使用方法-1" class="headerlink" title="传统的使用方法"></a>传统的使用方法</h2><h2 id="detach"><a href="#detach" class="headerlink" title="detach"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/_A5vA4Wn_aVQOeSOW2qAtg#">detach</a></h2><p>detach能够将数据从对应的网络中提取出来，能够阻断梯度回传。将一个变量从网络中分离出来，还是存放在原来的地方，只是将requires_grad设置为了False。</p>
<h2 id="NMS"><a href="#NMS" class="headerlink" title="NMS"></a><a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov3/issues/679">NMS</a></h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649990567860_0.png" alt="image.png"><br>这里以hard-nms为例：</p>
<ol>
<li>按类别进行遍历</li>
<li>选择当前类别中得分最高的框作为基准来进行nms操作</li>
<li>删除与基准iou超过阈值的候选框</li>
<li>从剩下的候选框中在选择最高得分的框作为一个新的基准重复操作</li>
</ol>
<h3 id="多种nms"><a href="#多种nms" class="headerlink" title="多种nms"></a>多种nms</h3><h4 id="Hard-nms"><a href="#Hard-nms" class="headerlink" title="Hard-nms"></a>Hard-nms</h4><p>直接删除相邻的同类别目标，密集目标的输出不友好。有多种不同的实现：</p>
<ol>
<li>or-nms: hard-nms的非官方实现形式，只支持cpu。</li>
<li>vision-nms: hard-nms的官方实现形式(c函数库)，可支持gpu（cuda），只支持单类别输入。</li>
<li>vision-batched-nms: hard-nms的官方实现形式(c函数库)，可支持gpu（cuda）,支持多类别输入。</li>
</ol>
<h4 id="Soft-nms"><a href="#Soft-nms" class="headerlink" title="Soft-nms"></a>Soft-nms</h4><p>hard-nms的官方实现形式(c函数库)，可支持gpu（cuda）,支持多类别输入。 [[soft-nms]]</p>
<h4 id="and-nms"><a href="#and-nms" class="headerlink" title="and-nms"></a>and-nms</h4><p>在hard-nms的逻辑基础上，增加是否为单独框的限制，删除没有重叠框的框（减少误检）。</p>
<h4 id="merge-nms"><a href="#merge-nms" class="headerlink" title="merge-nms"></a>merge-nms</h4><p>在hard-nms的逻辑基础上，增加是否为单独框的限制，删除没有重叠框的框（减少误检）。</p>
<h4 id="diou-nms"><a href="#diou-nms" class="headerlink" title="diou-nms"></a>diou-nms</h4><p>在hard-nms的基础上，用diou替换iou，里有参照diou的优势。</p>
<h3 id="CODE"><a href="#CODE" class="headerlink" title="CODE"></a>CODE</h3><h4 id="NMS-python实现"><a href="#NMS-python实现" class="headerlink" title="NMS python实现"></a>NMS python实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># method：可以选择使用哪种nms</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">non_max_suppression</span>(<span class="params">prediction, conf_thres=<span class="number">0.5</span>, nms_thres=<span class="number">0.5</span>, multi_cls=<span class="literal">True</span>, method=<span class="string">&#x27;vision&#x27;</span></span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Removes detections with lower object confidence score than &#x27;conf_thres&#x27;</span></span><br><span class="line"><span class="string">Non-Maximum Suppression to further filter detections.</span></span><br><span class="line"><span class="string">Returns detections with shape:</span></span><br><span class="line"><span class="string">(x1, y1, x2, y2, object_conf, conf, class)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># NMS methods https://github.com/ultralytics/yolov3/issues/679 &#x27;or&#x27;, &#x27;and&#x27;, &#x27;merge&#x27;, &#x27;vision&#x27;, &#x27;vision_batch&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Box constraints</span></span><br><span class="line">min_wh, max_wh = <span class="number">2</span>, <span class="number">4096</span>  <span class="comment"># (pixels) minimum and maximium box width and height</span></span><br><span class="line">output = [<span class="literal">None</span>] * <span class="built_in">len</span>(prediction)</span><br><span class="line"><span class="keyword">for</span> image_i, pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(prediction):</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply conf constraint</span></span><br><span class="line">pred = pred[pred[:, <span class="number">4</span>] &gt; conf_thres]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply width-height constraint</span></span><br><span class="line">pred = pred[(pred[:, <span class="number">2</span>:<span class="number">4</span>] &gt; min_wh).<span class="built_in">all</span>(<span class="number">1</span>) &amp; (pred[:, <span class="number">2</span>:<span class="number">4</span>] &lt; max_wh).<span class="built_in">all</span>(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># If none remain process next image</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(pred) == <span class="number">0</span>:</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute conf</span></span><br><span class="line">torch.sigmoid_(pred[..., <span class="number">5</span>:])</span><br><span class="line">pred[..., <span class="number">5</span>:] *= pred[..., <span class="number">4</span>:<span class="number">5</span>]  <span class="comment"># conf = obj_conf * cls_conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Box (center x, center y, width, height) to (x1, y1, x2, y2)</span></span><br><span class="line">box = xywh2xyxy(pred[:, :<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Detections matrix nx6 (xyxy, conf, cls)</span></span><br><span class="line"><span class="keyword">if</span> multi_cls <span class="keyword">or</span> conf_thres &lt; <span class="number">0.01</span>:</span><br><span class="line">i, j = (pred[:, <span class="number">5</span>:] &gt; conf_thres).nonzero().t()</span><br><span class="line">pred = torch.cat((box[i], pred[i, j + <span class="number">5</span>].unsqueeze(<span class="number">1</span>), j.<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:  <span class="comment"># best class only</span></span><br><span class="line">conf, j = pred[:, <span class="number">5</span>:].<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">pred = torch.cat((box, conf.unsqueeze(<span class="number">1</span>), j.<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply finite constraint</span></span><br><span class="line">pred = pred[torch.isfinite(pred).<span class="built_in">all</span>(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get detections sorted by decreasing confidence scores</span></span><br><span class="line">pred = pred[pred[:, <span class="number">4</span>].argsort(descending=<span class="literal">True</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Batched NMS</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">&#x27;vision_batch&#x27;</span>:</span><br><span class="line">output[image_i] = pred[torchvision.ops.boxes.batched_nms(pred[:, :<span class="number">4</span>], pred[:, <span class="number">4</span>], pred[:, <span class="number">5</span>], nms_thres)]</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># All other NMS methods</span></span><br><span class="line">det_max = []</span><br><span class="line">cls = pred[:, -<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> cls.unique():</span><br><span class="line">dc = pred[cls == c]  <span class="comment"># select class c</span></span><br><span class="line">n = <span class="built_in">len</span>(dc)</span><br><span class="line"><span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">det_max.append(dc)  <span class="comment"># No NMS required if only 1 prediction</span></span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line"><span class="keyword">elif</span> n &gt; <span class="number">500</span>:</span><br><span class="line">dc = dc[:<span class="number">500</span>]  <span class="comment"># limit to first 500 boxes: https://github.com/ultralytics/yolov3/issues/117</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">&#x27;vision&#x27;</span>:</span><br><span class="line">det_max.append(dc[torchvision.ops.boxes.nms(dc[:, :<span class="number">4</span>], dc[:, <span class="number">4</span>], nms_thres)])</span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">&#x27;or&#x27;</span>:  <span class="comment"># default</span></span><br><span class="line"><span class="keyword">while</span> dc.shape[<span class="number">0</span>]:</span><br><span class="line">det_max.append(dc[:<span class="number">1</span>])  <span class="comment"># save highest conf detection</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(dc) == <span class="number">1</span>:  <span class="comment"># Stop if we&#x27;re at the last detection</span></span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">iou = bbox_iou(dc[<span class="number">0</span>], dc[<span class="number">1</span>:])  <span class="comment"># iou with other boxes</span></span><br><span class="line">dc = dc[<span class="number">1</span>:][iou &lt; nms_thres]  <span class="comment"># remove ious &gt; threshold</span></span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">&#x27;and&#x27;</span>:  <span class="comment"># requires overlap, single boxes erased</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(dc) &gt; <span class="number">1</span>:</span><br><span class="line">iou = bbox_iou(dc[<span class="number">0</span>], dc[<span class="number">1</span>:])  <span class="comment"># iou with other boxes</span></span><br><span class="line"><span class="keyword">if</span> iou.<span class="built_in">max</span>() &gt; <span class="number">0.5</span>:</span><br><span class="line">det_max.append(dc[:<span class="number">1</span>])</span><br><span class="line">dc = dc[<span class="number">1</span>:][iou &lt; nms_thres]  <span class="comment"># remove ious &gt; threshold</span></span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">&#x27;merge&#x27;</span>:  <span class="comment"># weighted mixture box</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(dc):</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(dc) == <span class="number">1</span>:</span><br><span class="line">det_max.append(dc)</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">i = bbox_iou(dc[<span class="number">0</span>], dc) &gt; nms_thres  <span class="comment"># iou with other boxes</span></span><br><span class="line">weights = dc[i, <span class="number">4</span>:<span class="number">5</span>]</span><br><span class="line">dc[<span class="number">0</span>, :<span class="number">4</span>] = (weights * dc[i, :<span class="number">4</span>]).<span class="built_in">sum</span>(<span class="number">0</span>) / weights.<span class="built_in">sum</span>()</span><br><span class="line">det_max.append(dc[:<span class="number">1</span>])</span><br><span class="line">dc = dc[i == <span class="number">0</span>]</span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">&#x27;diounms&#x27;</span>:  <span class="comment"># use diou </span></span><br><span class="line"><span class="keyword">while</span> dc.shape[<span class="number">0</span>]:</span><br><span class="line">det_max.append(dc[:<span class="number">1</span>])  <span class="comment"># save highest conf detection</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(dc) == <span class="number">1</span>:  <span class="comment"># Stop if we&#x27;re at the last detection</span></span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">diou = bbox_iou(dc[<span class="number">0</span>], dc[<span class="number">1</span>:]，DIoU=<span class="literal">True</span>)  <span class="comment"># diou with other boxes</span></span><br><span class="line">dc = dc[<span class="number">1</span>:][diou &lt; nms_thres]  <span class="comment"># remove dious &gt; threshold</span></span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">&#x27;soft&#x27;</span>:  <span class="comment"># soft-NMS https://arxiv.org/abs/1704.04503</span></span><br><span class="line">sigma = <span class="number">0.5</span>  <span class="comment"># soft-nms sigma parameter</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(dc):</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(dc) == <span class="number">1</span>:</span><br><span class="line">det_max.append(dc)</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">det_max.append(dc[:<span class="number">1</span>])</span><br><span class="line">iou = bbox_iou(dc[<span class="number">0</span>], dc[<span class="number">1</span>:])  <span class="comment"># iou with other boxes</span></span><br><span class="line">dc = dc[<span class="number">1</span>:]</span><br><span class="line">dc[:, <span class="number">4</span>] *= torch.exp(-iou ** <span class="number">2</span> / sigma)  <span class="comment"># decay confidences</span></span><br><span class="line">dc = dc[dc[:, <span class="number">4</span>] &gt; conf_thres]  <span class="comment"># https://github.com/ultralytics/yolov3/issues/362</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(det_max):</span><br><span class="line">det_max = torch.cat(det_max)  <span class="comment"># concatenate</span></span><br><span class="line">output[image_i] = det_max[(-det_max[:, <span class="number">4</span>]).argsort()]  <span class="comment"># sort</span></span><br><span class="line"><span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h4 id="IOU计算"><a href="#IOU计算" class="headerlink" title="IOU计算"></a>IOU计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_iou</span>(<span class="params">box1, box2, x1y1x2y2=<span class="literal">True</span>, GIoU=<span class="literal">False</span>, DIoU=<span class="literal">False</span>, CIoU=<span class="literal">False</span></span>):</span><br><span class="line"></span><br><span class="line"><span class="comment"># Returns the IoU of box1 to box2. box1 is 4, box2 is nx4</span></span><br><span class="line">box2 = box2.t()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the coordinates of bounding boxes</span></span><br><span class="line"><span class="keyword">if</span> x1y1x2y2:  <span class="comment"># x1, y1, x2, y2 = box1</span></span><br><span class="line">b1_x1, b1_y1, b1_x2, b1_y2 = box1[<span class="number">0</span>], box1[<span class="number">1</span>], box1[<span class="number">2</span>], box1[<span class="number">3</span>]</span><br><span class="line">b2_x1, b2_y1, b2_x2, b2_y2 = box2[<span class="number">0</span>], box2[<span class="number">1</span>], box2[<span class="number">2</span>], box2[<span class="number">3</span>]</span><br><span class="line"><span class="keyword">else</span>:  <span class="comment"># x, y, w, h = box1</span></span><br><span class="line">b1_x1, b1_x2 = box1[<span class="number">0</span>] box1[<span class="number">2</span>] / <span class="number">2</span>, box1[<span class="number">0</span>] + box1[<span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">b1_y1, b1_y2 = box1[<span class="number">1</span>] box1[<span class="number">3</span>] / <span class="number">2</span>, box1[<span class="number">1</span>] + box1[<span class="number">3</span>] / <span class="number">2</span></span><br><span class="line">b2_x1, b2_x2 = box2[<span class="number">0</span>] box2[<span class="number">2</span>] / <span class="number">2</span>, box2[<span class="number">0</span>] + box2[<span class="number">2</span>] / <span class="number">2</span></span><br><span class="line">b2_y1, b2_y2 = box2[<span class="number">1</span>] box2[<span class="number">3</span>] / <span class="number">2</span>, box2[<span class="number">1</span>] + box2[<span class="number">3</span>] / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Intersection area</span></span><br><span class="line">inter = (torch.<span class="built_in">min</span>(b1_x2, b2_x2) torch.<span class="built_in">max</span>(b1_x1, b2_x1)).clamp(<span class="number">0</span>) * \</span><br><span class="line">(torch.<span class="built_in">min</span>(b1_y2, b2_y2) torch.<span class="built_in">max</span>(b1_y1, b2_y1)).clamp(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Union Area</span></span><br><span class="line">w1, h1 = b1_x2 b1_x1, b1_y2 b1_y1</span><br><span class="line">w2, h2 = b2_x2 b2_x1, b2_y2 b2_y1</span><br><span class="line">union = (w1 * h1 + <span class="number">1e-16</span>) + w2 * h2 inter</span><br><span class="line">iou = inter / union  <span class="comment"># iou</span></span><br><span class="line"><span class="keyword">if</span> GIoU <span class="keyword">or</span> DIoU <span class="keyword">or</span> CIoU:</span><br><span class="line">cw = torch.<span class="built_in">max</span>(b1_x2, b2_x2) torch.<span class="built_in">min</span>(b1_x1, b2_x1)  <span class="comment"># convex (smallest enclosing box) width</span></span><br><span class="line">ch = torch.<span class="built_in">max</span>(b1_y2, b2_y2) torch.<span class="built_in">min</span>(b1_y1, b2_y1)  <span class="comment"># convex height</span></span><br><span class="line"><span class="keyword">if</span> GIoU:  <span class="comment"># Generalized IoU https://arxiv.org/pdf/1902.09630.pdf</span></span><br><span class="line">c_area = cw * ch + <span class="number">1e-16</span>  <span class="comment"># convex area</span></span><br><span class="line"><span class="keyword">return</span> iou (c_area union) / c_area  <span class="comment"># GIoU</span></span><br><span class="line"><span class="keyword">if</span> DIoU <span class="keyword">or</span> CIoU:  <span class="comment"># Distance or Complete IoU https://arxiv.org/abs/1911.08287v1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># convex diagonal squared</span></span><br><span class="line">c2 = cw ** <span class="number">2</span> + ch ** <span class="number">2</span> + <span class="number">1e-16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># centerpoint distance squared</span></span><br><span class="line">rho2 = ((b2_x1 + b2_x2) (b1_x1 + b1_x2)) ** <span class="number">2</span> / <span class="number">4</span> + ((b2_y1 + b2_y2) (b1_y1 + b1_y2)) ** <span class="number">2</span> / <span class="number">4</span></span><br><span class="line"><span class="keyword">if</span> DIoU:</span><br><span class="line"><span class="keyword">return</span> iou rho2 / c2  <span class="comment"># DIoU</span></span><br><span class="line"><span class="keyword">elif</span> CIoU:  <span class="comment"># https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47</span></span><br><span class="line">v = (<span class="number">4</span> / math.pi ** <span class="number">2</span>) * torch.<span class="built_in">pow</span>(torch.atan(w2 / h2) torch.atan(w1 / h1), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">alpha = v / (<span class="number">1</span> iou + v)</span><br><span class="line"><span class="keyword">return</span> iou (rho2 / c2 + v * alpha)  <span class="comment"># CIoU</span></span><br><span class="line"><span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure>

<h2 id="正则化，规范化"><a href="#正则化，规范化" class="headerlink" title="正则化，规范化"></a>正则化，规范化</h2><p>在卷积操作中，经常会使用到BN操作，由于卷积操作其实也是数据在不同维度投影的操作。这个时候，如果执行从高维到低维的投影，若数据其中某一特征的数值特别大的话，那么它在整个误差计算的比重上就很大。所以将数据投影到低维空间之后，整个投影会去努力逼近数值最大的那一个特征，而忽略数值比较小的特征。<br>为了“公平”起见，防止过分捕捉某些数值大的特征，我们就可以先对每个特征先进行标准化处理，使得它们的大小都在相同的范围内。</p>
<h3 id="standardization"><a href="#standardization" class="headerlink" title="standardization"></a>standardization</h3><p>标准化，将训练集中某一列数值特征的值缩放成均值为0，方差为1的状态。</p>
<h3 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h3><p>归一化，将训练集中某一列数值特征的值缩放到0和1之间。</p>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><h2 id="平移不变性"><a href="#平移不变性" class="headerlink" title="平移不变性"></a>平移不变性</h2><h2 id="自定义OP"><a href="#自定义OP" class="headerlink" title="自定义OP"></a>自定义OP</h2><p>流程：</p>
<ol>
<li>注册OP</li>
<li>实现OP</li>
<li>创建python接口</li>
<li>实现OP梯度计算（不需要求导可以直接pass）</li>
</ol>
<h2 id="统计参数量"><a href="#统计参数量" class="headerlink" title="统计参数量"></a>统计参数量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">total_params = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> self.model.parameters():</span><br><span class="line">num_params = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> params.size():</span><br><span class="line">num_params *= x</span><br><span class="line">total_params += num_params</span><br></pre></td></tr></table></figure>

<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>由于神经网络中求梯度的时候，直接对矩阵进行十分复杂，但是所有的矩阵对矩阵的导数都是可以通过间接的方法，利用求标量导数的那些知识轻松求出来的。而这种间接求导数的方法就是维度分析。<br>通过维度分析和链式法则简化求导过程。</p>
<h3 id="维度分析"><a href="#维度分析" class="headerlink" title="维度分析"></a>维度分析</h3><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><h5 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h5><p>设某一层的Forward Pass为score&#x3D;XW + b,,<strong>X</strong>是<strong>NxD</strong>的矩阵，<strong>W</strong>是<strong>DxC</strong>的矩阵，b是1xC的矩阵，那么<strong>score</strong>就是一个<strong>NxC</strong>的矩阵。现在上层已经告诉你L对score的导数是多少了，我们求L对W和b的导数。</p>
<h5 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h5><p>首先<strong>Loss是一个标量</strong>，$\frac{dL}{dscore}$一定是一个<strong>NxC</strong>的矩阵，L对W的导数就就表示为：<br>$$<br>\frac{dL}{dW}&#x3D;\frac{dL}{dscore}\frac{dscore}{dW}<br>$$<br>这里score和W都是矩阵，直接进行矩阵对矩阵的求导复杂。</p>
<h5 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h5><p>其实我们没有必要直接求score对W的导数，我们可以利用另外两个导数间接地把<br>$\frac{dscore}{dW}$算出来。首先来看看它的形状。我们知道$\frac{dL}{dW}$一定是DxC的(形状和W一致)，而$\frac{dL}{dscore}$是NxC的，所以$\frac{dscore}{dW}$一定是DxN的，因为(DxN)x(NxC)→(DxC),按计算可以看出，求导公式的正式写法应该是：$\frac{dL}{dW}&#x3D;\frac{dscore}{dW}\frac{dL}{dscore}$。<br>既然$score&#x3D;XW+b$，如果都是标量的话，score对W求导，本身就是X；X是NxD的，所以我们需要DxN的形状的数据，顺其自然我们只需要转置一下就可以得到：<br>$$<br>\frac{dL}{dW}&#x3D;X^T\frac{dL}{dscore}<br>$$<br>完事。<br>这样就避免了直接去用$\frac{dscore_{11}}{dW_{11}}$这种细枝末节的一个一个元素求导的方式推导$\frac{dscore}{dW}$,这就是神经网络种求取导数的正确姿势。</p>
<blockquote>
<p>实现的关键在于Loss是一个标量，而标量对一个矩阵求导，其大小和这个矩阵的大小永远是一样的。</p>
</blockquote>
<h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><blockquote>
<p>链式法则在神经网络中十分重要！</p>
</blockquote>
<h2 id="BN集合"><a href="#BN集合" class="headerlink" title="BN集合"></a>BN集合</h2><p>BN依据不同的方式有多种不同的形式</p>
<h3 id="traditional-BN"><a href="#traditional-BN" class="headerlink" title="traditional BN"></a>traditional BN</h3><p>[[BN]]</p>
<blockquote>
<p>BatchNorm是针对单个神经元进行的，利用网络训练时一个 mini-batch 的数据来计算该神经元的均值和方差。所以对Batch有强依赖。CxHxW相当于有CxHxW这么多个神经元。</p>
</blockquote>
<p>步骤：</p>
<ol>
<li>计算&#x2F;更新均值</li>
<li>计算&#x2F;更新方差</li>
<li>使用均值和方差对每个元素标准化</li>
<li>BN的可学习参数就是线性变化</li>
</ol>
<h3 id="统计对象的魔改"><a href="#统计对象的魔改" class="headerlink" title="统计对象的魔改"></a>统计对象的魔改</h3><h4 id="Layer-Normization"><a href="#Layer-Normization" class="headerlink" title="Layer Normization"></a>Layer Normization</h4><p>计算<strong>C H W</strong>上的均值和方差，这样的归一化方式，使 batch 中的每个样本均可利用其<strong>本身的数据</strong>的进行归一化操作。<br>更<strong>高效方便</strong>，也不存在更新均值和方差时，batch 内均值和方差不稳定的问题。<br>在 <strong>NLP</strong> 中（Transformer）比较常用，CNN 上作用不大，但是随着Transformer的跨界，LN和MLP的组合变得十分常见。</p>
<h4 id="Instance-Normization"><a href="#Instance-Normization" class="headerlink" title="Instance Normization"></a>Instance Normization</h4><p>计算 (<strong>H, W</strong>) 上的均值和方差，作用于每个样本的每个通道上的归一化方法<br>由于其计算均值和方差的粒度较细，在 low-level 的任务中（例如风格迁移）通常表现突出。在 GAN 网络的生成器中使用，IN 会降低生成图像中的网格感，使其更加自然。</p>
<h4 id="Group-Normization"><a href="#Group-Normization" class="headerlink" title="Group Normization"></a>Group Normization</h4><p>LN和IN的均衡操作。<br>在<strong>一定数目的 channel</strong> 上进行归一化（即计算 <strong>(c, H, W)</strong> 上的均值和方差）。当 batch size 小于 8 的时候，通常使用 Group Normalization 代替 Batch Normalization，对训练好的模型 fineture，效果还不错。</p>
<h4 id="Switchable-Normalization"><a href="#Switchable-Normalization" class="headerlink" title="Switchable Normalization"></a>Switchable Normalization</h4><p>BN、LN、IN按一定的比例加权组合，权重系数通过网络学习。<br>速度慢。</p>
<h4 id="Sync-Batch-Normalization"><a href="#Sync-Batch-Normalization" class="headerlink" title="Sync Batch Normalization"></a><a target="_blank" rel="noopener" href="https://github.com/chrisway613/Synchronized-BatchNormalization">Sync Batch Normalization</a></h4><p>跨卡实现同步BN操作，SN 能够帮助我们屏蔽多卡训练的分布式细节，将分散在每个 GPU 上的 batch 合并，视为一个机器上的一个 batch。<br>其关键是在前向运算的时候，计算并同步全局（所有 GPU 上 batch）的均值和方差；在反向运算时候，同步相应的全局梯度。</p>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><h6 id="2次同步"><a href="#2次同步" class="headerlink" title="2次同步"></a>2次同步</h6><p>先同步各卡的均值，计算出全局的均值，然后同步给各卡，接着各卡同步计算方差。<br><strong>缺点</strong>：消耗资源，且影响训练速度。</p>
<h6 id="１次同步"><a href="#１次同步" class="headerlink" title="１次同步"></a>１次同步</h6><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649997819467_0.png" alt="image.png"><br>其中ｍ为各卡拿到的数据批次大小$(\frac{batch*size}{nGPU})$。<br>由上可知，<strong>每张</strong>卡计算出$\sum_{i&#x3D;1}^{m}X_i$和$\sum_{i&#x3D;1}^{m}x_i^2$，然后进行求和，即可计算出<strong>全局的方差</strong>，同时，全局的均值可通过<strong>各卡</strong>的$\sum_{i&#x3D;1}^{m}x_i$同步求和得到，一次同步就可实现全局均值和方差的计算。<br>关键在于重载 [[replicate]] 方法，<strong>原生的该方法只是将模型在每张卡上复制一份</strong>，并且没有建立起联系，而我们的 <strong>SyncBN 是需要进行同步</strong>的，因此需要重载该方法，让各张卡上的SyncBN 通过某种数据结构和同步机制建立起联系。</p>
<h4 id="Sparse-Switchable-Normalization"><a href="#Sparse-Switchable-Normalization" class="headerlink" title="Sparse Switchable Normalization"></a>Sparse Switchable Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649998746111_0.png" alt="image.png"><br>Switchable Normalization 的改进版。通过稀疏约束，使用 sparsestmax 替代了 softmax，在保持性能的同时减少了 switchable 的计算量，增加了鲁棒性。</p>
<h4 id="Region-Normalization"><a href="#Region-Normalization" class="headerlink" title="Region Normalization"></a>Region Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649998810544_0.png" alt="image.png"><br>Region Normalization 在 Batch Normalization 的基础上进一步对 (N, H, W) 中的 (H, W) 进行了划分。Region Normalization 根据输入掩码将空间像素划分为不同的区域，并计算每个区域的均值和方差以进行归一化。实现时只需将输入先进行 mask，再输入 BN 即可。</p>
<h4 id="Local-Context-Normalization"><a href="#Local-Context-Normalization" class="headerlink" title="Local Context Normalization"></a>Local Context Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999016347_0.png" alt="image.png"><br>Local Context Normalization 在 Layer Normalization 的基础上对 (C, H, W) 进行了划分。Local Context Normalization 根据每个特征值所在的局部邻域，计算邻域的均值和方差以进行归一化。实现时作者借助了空洞卷积获取 window 内均值和方差，比较巧妙。</p>
<h4 id="Channel-Normalization"><a href="#Channel-Normalization" class="headerlink" title="Channel Normalization"></a>Channel Normalization</h4><p>在 channel 维度求统计量，对 tensor 进行归一化。</p>
<h4 id="Domain-Specific-Batch-Normalization"><a href="#Domain-Specific-Batch-Normalization" class="headerlink" title="Domain-Specific Batch Normalization"></a>Domain-Specific Batch Normalization</h4><p>Domain-Specific Batch Normalization 与 Split Batch Normalization 类似，都针对 sample 的不同进行了分组。不同的是，在 Domain adaptation 任务中，Domain-Specific Batch Normalization 选择对 source domain 和 target domain 进行分组。在计算时，干脆创建了两个 Batch Normalization，根据 domain 选择相应的分支输入。</p>
<h3 id="规范化方式的魔改"><a href="#规范化方式的魔改" class="headerlink" title="规范化方式的魔改"></a>规范化方式的魔改</h3><h4 id="Filter-Response-Normalization"><a href="#Filter-Response-Normalization" class="headerlink" title="Filter Response Normalization"></a>Filter Response Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999050127_0.png" alt="image.png"><br>对Instance Normalization的改进。在 (H, W) 维度上计算统计量。不同的是，Filter Response Normalization 并不计算平均值，在规范化的过程中，直接使用每个元素除以 (H, W) 维度二范数的平均值。由于缺少减去均值的操作，因此归一化的结果可能会产生偏移（不以 0 为中心），这对于后续的 ReLU 激活是不利的。因此，作者还提出了配套的激活函数 TLU。（其实 TLU 单独使用也挺好用的）</p>
<h4 id="Extended-Batch-Normalization"><a href="#Extended-Batch-Normalization" class="headerlink" title="Extended Batch Normalization"></a>Extended Batch Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999073532_0.png" alt="image.png"><br>Extended Batch Normalization 改进了 Batch Normalization。虽然仍使用均值和方差进行规范化，不同之处在于，Extended Batch Normalization 在 (N, H, W) 的维度上求平均值，在 (N, C, H, W) 的维度上求方差。直观上看，统计元素数量的增多使得方差更为稳定，因而能够在小 batch 上取得较好的效果。</p>
<h4 id="Kalman-Normalization"><a href="#Kalman-Normalization" class="headerlink" title="Kalman Normalization"></a>Kalman Normalization</h4><p>Kalman Normalization 改进了 Batch Normalization，使得不同层之间的统计量得以相互关联。在 Kalman Normalization，当前归一化层的均值和方差是通过和上一个归一化层的均值和方差进行卡尔曼滤波得到的（可以简单理解当前状态为加权历史状态），其在大尺度物体检测和风格任务上都取得了较好的效果。</p>
<h4 id="L1-Norm-Batch-Normalization"><a href="#L1-Norm-Batch-Normalization" class="headerlink" title="L1-Norm Batch Normalization"></a>L1-Norm Batch Normalization</h4><p>L1-Norm Batch Normalization 将 Batch Normalization 中的求方差换成了 L1 范数。在 GAN 中表现出了较好的效果。</p>
<h4 id="Cosine-Normalization"><a href="#Cosine-Normalization" class="headerlink" title="Cosine Normalization"></a>Cosine Normalization</h4><p>提出使用 cosine similarity 进行归一化。</p>
<h3 id="仿射变换方式的魔改"><a href="#仿射变换方式的魔改" class="headerlink" title="仿射变换方式的魔改"></a>仿射变换方式的魔改</h3><p>仿射变换的方式给人的感觉就是将BN也往网络上改。</p>
<h4 id="Conditional-Batch-x2F-Instance-Normalization"><a href="#Conditional-Batch-x2F-Instance-Normalization" class="headerlink" title="Conditional Batch&#x2F;Instance Normalization"></a>Conditional Batch&#x2F;Instance Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999153042_0.png" alt="image.png"><br>Conditional Batch Normalization 在 Batch Normalization 的基础上改进了仿射变换的部分。它使用 LSTM 和多层感知器，将自然语言映射为一组特征向量，作为仿射变换的权重 γ 和偏置 β 引导后续任务。Conditional Instance Normalization 则是对 style 信息进行编码，使用在风格迁移中（正如之前所介绍的，Instance Normalization 在 low-level 任务中更有优势）。笔者认为，二者都可以看作使用 attention 机制引入了外部信息。</p>
<h4 id="Adaptive-Instance-Normalization"><a href="#Adaptive-Instance-Normalization" class="headerlink" title="Adaptive Instance Normalization"></a>Adaptive Instance Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999171443_0.png" alt="image.png"><br>Adaptive Instance Normalization 进一步放宽了权重 γ 和偏置 β 在风格迁移中的估计方法。作者提出的方法不再需要一个单独的 style 特征提取模块，对于给定的风格图像和原始图像都使用相同的 backbone （VGG）提取特征，用风格图像算出权重 γ 和偏置 β ，用于原始图像的仿射变换中。</p>
<h4 id="Adaptive-Convolution-based-Normalization"><a href="#Adaptive-Convolution-based-Normalization" class="headerlink" title="Adaptive Convolution-based Normalization"></a>Adaptive Convolution-based Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999194065_0.png" alt="image.png"><br>Adaptive Convolution-based Normalization 是 Adaptive Instance Normalization 的改良版。不同之处在于，Adaptive Convolution-based Normalization 在仿射变换的过程中，使用一个动态的卷积层完成。这时的仿射变换已经漏出了卷积的獠牙，传统意义上的仿射变换名存实亡。</p>
<h4 id="Spatially-Adaptive-Normalization"><a href="#Spatially-Adaptive-Normalization" class="headerlink" title="Spatially-Adaptive Normalization"></a>Spatially-Adaptive Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999215139_0.png" alt="image.png"><br>Spatially-Adaptive Normalization 在使用均值和方差将每个元素标准化到 [0,1] 后，在仿射变换层融入了 mask 引导的 attention 机制。作者首先使用卷积层对 mask 进行变换，在得到的 feature map 上分别使用两个卷积层得到权重 γ 和偏置 β 的估计。最后使用权重 γ 和偏置 β 的估计对归一化的结果进行 element-wise 的乘加操作，完成归一化。</p>
<h4 id="Region-Adaptive-Normalization"><a href="#Region-Adaptive-Normalization" class="headerlink" title="Region-Adaptive Normalization"></a>Region-Adaptive Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999235742_0.png" alt="image.png"><br>Region-Adaptive Normalization 在 Spatially-Adaptive Normalization 的基础上进行了 style map 和 segmentation mask 两个 branch 的 fusion。分别使用 style map 和 mask 套用 Spatially-Adaptive Normalization 得到权重 γ 和偏置 β 的估计，再将两个 branch 的估计加权平均，得到最终的估计。而后进行仿射变换完成归一化。</p>
<h4 id="Instance-Enhancement-Batch-Normalization"><a href="#Instance-Enhancement-Batch-Normalization" class="headerlink" title="Instance Enhancement Batch Normalization"></a>Instance Enhancement Batch Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999254700_0.png" alt="image.png"><br>Instance Enhancement Batch Normalization 对权重 γ 的估计则更为简单粗暴。在使用时，不需要引入 mask 等额外信息做引导，由网络自适应地学习。作者借鉴了 SENet 的思路，通过池化、变换、Sigmoid 得到一组权重 γ 。这使得 Instance Enhancement Batch Normalization 具有很强的通用性，尽管参数量有所提升，但是即插即用无痛涨点。</p>
<h4 id="Attentive-Normalization"><a href="#Attentive-Normalization" class="headerlink" title="Attentive Normalization"></a>Attentive Normalization</h4><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1649999273235_0.png" alt="image.png"><br>Attentive Normalization 与 Instance Enhancement Batch Normalization 的方法类似。笔者认为 Attentive Normalization 这个名称似乎更加形象一些。</p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="保存加载模型"><a href="#保存加载模型" class="headerlink" title="保存加载模型"></a>保存加载模型</h2><h3 id="保存加载模型基本用法"><a href="#保存加载模型基本用法" class="headerlink" title="保存加载模型基本用法"></a>保存加载模型基本用法</h3><h4 id="整个模型"><a href="#整个模型" class="headerlink" title="整个模型"></a>整个模型</h4><p>保存整个网络模型（<strong>网络结构+权重参数</strong>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, <span class="string">&#x27;net.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>直接加载<strong>整个网络模型</strong>（可能比较耗时）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(<span class="string">&#x27;net.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="只针对模型参数"><a href="#只针对模型参数" class="headerlink" title="只针对模型参数"></a>只针对模型参数</h4><p>只保存模型的权重参数（速度快，占内存少）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;net_params.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>因为我们只保存了模型的参数，所以需要先定义一个网络对象，然后再加载模型参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 构建一个网络结构</span></span><br><span class="line">model = ClassNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型参数加载到新模型中</span></span><br><span class="line">state_dict = torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>)</span><br><span class="line">model.load_state_dict(state_dict)</span><br></pre></td></tr></table></figure>

<h3 id="加载部分参数"><a href="#加载部分参数" class="headerlink" title="加载部分参数"></a>加载部分参数</h3><p>模型微调操作，能够满足模型的按需加载操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pretrained_dict = torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>)<span class="comment">#加载模型</span></span><br><span class="line">net = Net()<span class="comment"># 获取模型</span></span><br><span class="line">net_state_dict = net.state_dict()<span class="comment">#获取模型的权重字典</span></span><br><span class="line">pretrained_dict_1 = &#123;k:v <span class="keyword">for</span> k,v <span class="keyword">in</span> pretrained_dict.items() <span class="keyword">if</span> k <span class="keyword">in</span> net_state_dict&#125;<span class="comment">#将pretrained_dict里不属于net_state_dict的键剔除掉</span></span><br><span class="line">net_state_dict.update(pretrained_dict_1)<span class="comment">#使用筛选后的字典对模型参数进行更新</span></span><br><span class="line">net.load_state_dict(net_state_dict)<span class="comment"># 将更新后的参数字典“放”回网络中</span></span><br></pre></td></tr></table></figure>

<h3 id="pkl文件"><a href="#pkl文件" class="headerlink" title="pkl文件"></a>pkl文件</h3><p>保存加载的 net.pkl 其实一个字典，通常包含如下内容：</p>
<ol>
<li><strong>网络结构</strong>：输入尺寸、输出尺寸以及隐藏层信息，以便能够在加载时重建模型。</li>
<li><strong>模型的权重参数</strong>：包含各网络层训练后的可学习参数，可以在模型实例上调用 <code>state_dict()</code> 方法来获取，比如前面介绍只保存模型权重参数时用到的 <code>model.state_dict()</code>。</li>
<li><strong>优化器参数</strong>：有时保存模型的参数需要稍后接着训练，那么就必须保存优化器的状态和所其使用的超参数，也是在优化器实例上调用 <code>state_dict()</code> 方法来获取这些参数。</li>
<li>其他信息：有时我们需要保存一些其他的信息，比如 <code>epoch</code>，<code>batch_size</code> 等超参数。</li>
</ol>
<h3 id="自定义模型"><a href="#自定义模型" class="headerlink" title="自定义模型"></a>自定义模型</h3><h4 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h4><p>通过pkl文件格式，我们就可以自定义需要保存的内容，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># saving a checkpoint assuming the network class named ClassNet</span></span><br><span class="line">checkpoint = &#123;<span class="string">&#x27;model&#x27;</span>: ClassNet(),</span><br><span class="line"><span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line"><span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line"><span class="string">&#x27;epoch&#x27;</span>: epoch&#125;</span><br><span class="line">torch.save(checkpoint, <span class="string">&#x27;checkpoint.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>上面的 checkpoint 是个字典，里面有4个键值对，分别表示网络模型的不同信息。</strong></p>
<h4 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_checkpoint</span>(<span class="params">filepath</span>):</span><br><span class="line">checkpoint = torch.load(filepath)</span><br><span class="line">model = checkpoint[<span class="string">&#x27;model&#x27;</span>]  <span class="comment"># 提取网络结构</span></span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])  <span class="comment"># 加载网络权重参数</span></span><br><span class="line">optimizer = TheOptimizerClass()</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])  <span class="comment"># 加载优化器参数</span></span><br><span class="line"><span class="keyword">for</span> parameter <span class="keyword">in</span> model.parameters():</span><br><span class="line">parameter.requires_grad = <span class="literal">False</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">return</span> model</span><br><span class="line">model = load_checkpoint(<span class="string">&#x27;checkpoint.pkl&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果加载模型只是为了进行推理测试，则将每一层的 requires_grad 置为 False，即固定这些权重参数；还需要调用 model.eval() 将模型置为测试模式，主要是将 dropout 和 batch normalization 层进行固定，否则模型的预测结果每次都会不同。</p>
</blockquote>
<blockquote>
</blockquote>
<p>如果希望继续训练，则调用 model.train()，以确保网络模型处于训练模式。</p>
<h3 id="state-dict"><a href="#state-dict" class="headerlink" title="state_dict"></a>state_dict</h3><blockquote>
<p>state_dict() 也是一个Python字典对象，model.state_dict() 将每一层的可学习参数映射为参数矩阵，其中只包含具有可学习参数的层(卷积层、全连接层等)。</p>
</blockquote>
<h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Define model</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TheModelClass</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="built_in">super</span>(TheModelClass, self).__init__()</span><br><span class="line">self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>)</span><br><span class="line">self.bn = nn.BatchNorm2d(<span class="number">8</span>)</span><br><span class="line">self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">x = self.bn(x)</span><br><span class="line">x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">x = F.relu(self.fc1(x))</span><br><span class="line">x = F.relu(self.fc2(x))</span><br><span class="line">x = self.fc3(x)</span><br><span class="line"><span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize model</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型，但是没有传入数据，无所谓，因为模型的参数原来就不依附数据</span></span><br><span class="line">model = TheModelClass()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize optimizer</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model&#x27;s state_dict:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> param_tensor <span class="keyword">in</span> model.state_dict():</span><br><span class="line"><span class="built_in">print</span>(param_tensor, <span class="string">&quot;\t&quot;</span>, model.state_dict()[param_tensor].size())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Optimizer&#x27;s state_dict:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> var_name <span class="keyword">in</span> optimizer.state_dict():</span><br><span class="line"><span class="built_in">print</span>(var_name, <span class="string">&quot;\t&quot;</span>, optimizer.state_dict()[var_name])</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Model<span class="string">&#x27;s state_dict:</span></span><br><span class="line"><span class="string">conv1.weight            torch.Size([8, 3, 5, 5])</span></span><br><span class="line"><span class="string">conv1.bias              torch.Size([8])</span></span><br><span class="line"><span class="string">bn.weight               torch.Size([8])</span></span><br><span class="line"><span class="string">bn.bias                 torch.Size([8])</span></span><br><span class="line"><span class="string">bn.running_mean         torch.Size([8])</span></span><br><span class="line"><span class="string">bn.running_var          torch.Size([8])</span></span><br><span class="line"><span class="string">bn.num_batches_tracked  torch.Size([])</span></span><br><span class="line"><span class="string">conv2.weight            torch.Size([16, 8, 5, 5])</span></span><br><span class="line"><span class="string">conv2.bias              torch.Size([16])</span></span><br><span class="line"><span class="string">fc1.weight              torch.Size([120, 400])</span></span><br><span class="line"><span class="string">fc1.bias                torch.Size([120])</span></span><br><span class="line"><span class="string">fc2.weight              torch.Size([10, 120])</span></span><br><span class="line"><span class="string">fc2.bias                torch.Size([10])</span></span><br><span class="line"><span class="string">Optimizer&#x27;</span>s state_dict:</span><br><span class="line">state            &#123;&#125;</span><br><span class="line">param_groups     [&#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.001</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>, <span class="string">&#x27;dampening&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;weight_decay&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;nesterov&#x27;</span>: <span class="literal">False</span>, <span class="string">&#x27;params&#x27;</span>: [<span class="number">139805696932024</span>, <span class="number">139805483616008</span>, <span class="number">139805483616080</span>, <span class="number">139805483616152</span>, <span class="number">139805483616440</span>, <span class="number">139805483616512</span>, <span class="number">139805483616584</span>, <span class="number">139805483616656</span>, <span class="number">139805483616728</span>, <span class="number">139805483616800</span>]&#125;]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以看到 model.state_dict() 保存了卷积层，BatchNorm层和最大池化层的信息；而 optimizer.state_dict() 则保存的优化器的状态和相关的超参数。</p>
</blockquote>
<h3 id="跨设备保存加载模型"><a href="#跨设备保存加载模型" class="headerlink" title="跨设备保存加载模型"></a>跨设备保存加载模型</h3><h4 id="Save-on-GPU-Load-on-CPU"><a href="#Save-on-GPU-Load-on-CPU" class="headerlink" title="Save on GPU, Load on CPU"></a>Save on GPU, Load on CPU</h4><p>在 CPU 上加载在 GPU 上训练并保存的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model = TheModelClass()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load all tensors onto the CPU device</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>, map_location=device))</span><br><span class="line"></span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>, map_location=<span class="string">&#x27;cpu&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p><strong>map_location：</strong>一个公式、torch.device、字符串或者一个字典，用来明确如何去重新映射存储位置。<br>官方示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load all tensors onto the CPU</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load all tensors onto the CPU, using a function</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load all tensors onto GPU 1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=<span class="keyword">lambda</span> storage, loc: storage.cuda(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Map tensors from GPU 1 to GPU 0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=&#123;<span class="string">&#x27;cuda:1&#x27;</span>:<span class="string">&#x27;cuda:0&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>不需要将模型迁移到device上，因为一开始载入都是载入到内存先的，所以不需要迁移操作。</p>
</blockquote>
<h4 id="Save-on-GPU-Load-on-GPU"><a href="#Save-on-GPU-Load-on-GPU" class="headerlink" title="Save on GPU, Load on GPU"></a>Save on GPU, Load on GPU</h4><p>在 GPU 上加载在 GPU 上训练并保存的模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">model = TheModelClass()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>))</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>需要将模型和数据都迁移到显存上，才能开始训练。</p>
</blockquote>
<p>在这里使用 map_location 参数不起作用，要使用 <strong><a target="_blank" rel="noopener" href="http://model.to/">model.to</a>(torch.device(“cuda”))</strong> 将模型转换为CUDA优化的模型。<br>还需要对将要输入模型的数据调用 **data &#x3D; <a target="_blank" rel="noopener" href="http://data.to/">data.to</a>(device)**，即将数据从CPU转移到GPU。请注意，调用 my_tensor.to(device) 会返回一个 my_tensor 在 GPU 上的副本，它不会覆盖 my_tensor。因此需要手动覆盖张量：my_tensor &#x3D; my_tensor.to(device)。</p>
<h4 id="Save-on-CPU-Load-on-GPU"><a href="#Save-on-CPU-Load-on-GPU" class="headerlink" title="Save on CPU, Load on GPU"></a>Save on CPU, Load on GPU</h4><p>在 GPU 上加载在 CPU 上训练并保存的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">model = TheModelClass()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>, map_location=<span class="string">&quot;cuda:0&quot;</span>))</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法二</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda：0&quot;</span>)</span><br><span class="line">model = TheModelClass()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;net_params.pkl&#x27;</span>))</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>当加载<strong>单个</strong>包含GPU tensors的模型的设备时，这些tensors 会被默认加载到GPU上，不过是同一个GPU设备。<br>当有<strong>多个GPU设备</strong>时，可以通过将 map_location 设定为 cuda:device_id 来指定使用哪一个GPU设备，上面例子是指定编号为0的GPU设备。</p>
<h3 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h3><h4 id="判断cuda是否可用"><a href="#判断cuda是否可用" class="headerlink" title="判断cuda是否可用"></a>判断cuda是否可用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>

<h4 id="获取gpu数量"><a href="#获取gpu数量" class="headerlink" title="获取gpu数量"></a>获取gpu数量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.device_count())</span><br></pre></td></tr></table></figure>

<h4 id="获取gpu名字"><a href="#获取gpu名字" class="headerlink" title="获取gpu名字"></a>获取gpu名字</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.get_device_name(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h4 id="返回当前gpu设备索引，默认从0开始"><a href="#返回当前gpu设备索引，默认从0开始" class="headerlink" title="返回当前gpu设备索引，默认从0开始"></a>返回当前gpu设备索引，默认从0开始</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.current_device()</span><br></pre></td></tr></table></figure>

<h4 id="查看tensor或者model在哪块GPU上"><a href="#查看tensor或者model在哪块GPU上" class="headerlink" title="查看tensor或者model在哪块GPU上"></a>查看tensor或者model在哪块GPU上</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">0</span>]).get_device()</span><br></pre></td></tr></table></figure>

<h4 id="将数据和模型从cpu迁移到gpu上"><a href="#将数据和模型从cpu迁移到gpu上" class="headerlink" title="将数据和模型从cpu迁移到gpu上"></a>将数据和模型从cpu迁移到gpu上</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法一：</span></span><br><span class="line"><span class="keyword">if</span> use_cuda:</span><br><span class="line">data = data.cuda()</span><br><span class="line">model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二：</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#或</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">data = data.to(device)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<h2 id="no-grad"><a href="#no-grad" class="headerlink" title="no_grad"></a>no_grad</h2><p>一般来说，我们在进行模型训练的过程中，因为要监控模型的性能，在跑完若干个epoch训练之后，需要进行一次在验证集[4]上的性能验证。一般来说，在验证或者是测试阶段，因为只是需要跑个前向传播(forward)就足够了，<strong>因此不需要保存变量的梯度</strong>。<br><strong>问题：</strong>保存梯度是需要额外显存或者内存进行保存的，占用了空间，有时候还会在验证阶段导致OOM(Out Of Memory)错误，因此我们在验证和测试阶段，最好显式地取消掉模型变量的梯度。通过no_grad实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># here train the model, just skip the codes</span></span><br><span class="line">model.<span class="built_in">eval</span>() <span class="comment"># here we start to evaluate the model</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> eval_data:</span><br><span class="line">data, label = each</span><br><span class="line">logit = model(data)</span><br><span class="line"><span class="meta">... </span><span class="comment"># here we just skip the codes</span></span><br></pre></td></tr></table></figure>
<p>在pytorch0.4之前的版本通过volatile&#x3D;True来进行设置。</p>
<h2 id="model-eval"><a href="#model-eval" class="headerlink" title="model.eval"></a>model.eval</h2><blockquote>
<p>在模型中有BN层或者dropout层时，在训练阶段和测试阶段必须显式指定train()和eval()。</p>
</blockquote>
<p>我们的模型中经常会有一些子模型，其在<strong>训练时候和测试时候的参数是不同的</strong>，比如dropout[6]中的丢弃率和Batch Normalization[5]中的$\gamma$和$\beta$等，这个时候我们就需要显式地指定不同的阶段（训练或者测试），在pytorch中我们通过model.train()和model.eval()进行显式指定，具体如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">model = CNNNet(params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># here we start the training</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> train_data:</span><br><span class="line">data, label = each</span><br><span class="line">logit = model(data)</span><br><span class="line">loss = criterion(logit, label)</span><br><span class="line"><span class="meta">... </span><span class="comment"># just skip</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># here we start the evaluation</span></span><br><span class="line">model.<span class="built_in">eval</span>() </span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment"># we dont need grad in eval phase</span></span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> eval_data:</span><br><span class="line">data, label = each</span><br><span class="line">logit = model(data)</span><br><span class="line">loss = criterion(logit, label)</span><br><span class="line"><span class="meta">... </span><span class="comment"># just skip</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>从这里看model.train()和model.eval()函数并不执行模型，只是对其中的BN和Dropout进行设置。</p>
</blockquote>
<h2 id="retain-graph"><a href="#retain-graph" class="headerlink" title="retain_graph"></a>retain_graph</h2><p>在对一个损失进行反向传播时，在pytorch中调用out.backward()即可实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">net</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="built_in">super</span>().__init__()</span><br><span class="line">self.fc1 = nn.Linear(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">self.act = nn.ReLU()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputv</span>):</span><br><span class="line"><span class="keyword">return</span> self.act(self.fc1(inputv))</span><br><span class="line">n = net()</span><br><span class="line">opt = torch.optim.Adam(n.parameters(),lr=<span class="number">3e-4</span>)</span><br><span class="line">inputv = torch.tensor(np.random.normal(size=(<span class="number">4</span>,<span class="number">10</span>))).<span class="built_in">float</span>()</span><br><span class="line">output = n(inputv)</span><br><span class="line">target = torch.tensor(np.ones((<span class="number">4</span>,<span class="number">2</span>))).<span class="built_in">float</span>()</span><br><span class="line">loss = nn.functional.mse_loss(output, target)</span><br><span class="line">loss.backward() <span class="comment"># here we calculate the gradient w.r.t the leaf</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>w.r.t按这里的翻译就是叶子的梯度。</p>
</blockquote>
<p>对loss进行反向传播就可以获得$\partial loss&#x2F;\partial w_{i,j}$,即是损失对于每个叶子节点的梯度。在.backward这个API文档中，有多个参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">backward(gradient=<span class="literal">None</span>, retain_graph=<span class="literal">None</span>, create_graph=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>这里我们关注的是retain_graph这个参数，这个参数如果为<strong>False或者None</strong>则在反向传播完后，就<strong>释放掉构建出来的graph</strong>，如果为<strong>True则不对graph进行释放</strong>。</p>
<h3 id="为什么不释放"><a href="#为什么不释放" class="headerlink" title="为什么不释放"></a>为什么不释放</h3><blockquote>
<p>存在多个loss的时候，很有用。</p>
</blockquote>
<h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h4><p>首先构造graph：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">a = Variable(torch.rand(<span class="number">1</span>, <span class="number">4</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = a**<span class="number">2</span></span><br><span class="line">c = b*<span class="number">2</span></span><br><span class="line">d = c.mean()</span><br><span class="line">e = c.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<p>如图：<br><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1650001096236_0.png" alt="image.png"><br>当我们第一次使用$d.backward()$对末节点d进行求梯度，这样在执行完反向传播之后，因为没有显式地要求它保留graph，系统对graph内存进行释放，<strong>如果下一步需要对节点e进行求梯度，那么将会因为没有这个graph而报错</strong>。因此有例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d.backward(retain_graph=<span class="literal">True</span>) <span class="comment"># fine</span></span><br><span class="line">e.backward(retain_graph=<span class="literal">True</span>) <span class="comment"># fine</span></span><br><span class="line">d.backward() <span class="comment"># also fine</span></span><br><span class="line">e.backward() <span class="comment"># error will occur!</span></span><br></pre></td></tr></table></figure>
<p>利用这个性质在某些场景是有作用的，比如在<strong>对抗生成网络GAN</strong>中需要先对某个模块比如生成器进行训练，后对判别器进行训练，这个时候整个网络就会存在<strong>两个以上的loss</strong>，例子如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">G_loss = ...</span><br><span class="line">D_loss = ...</span><br><span class="line">opt.zero_grad() <span class="comment"># 对所有梯度清0</span></span><br><span class="line">D_loss.backward(retain_graph=<span class="literal">True</span>) <span class="comment"># 保存graph结构，后续还要用</span></span><br><span class="line">opt.step() <span class="comment"># 更新梯度，只更新D的，因为只有D的不为0</span></span><br><span class="line">opt.zero_grad() <span class="comment"># 对所有梯度清0</span></span><br><span class="line">G_loss.backward(retain_graph=<span class="literal">False</span>) <span class="comment"># 不保存graph结构了，可以释放graph，</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下一个迭代中通过forward还可以build出来的</span></span><br><span class="line">opt.step() <span class="comment"># 更新梯度，只更新G的，因为只有G的不为0</span></span><br></pre></td></tr></table></figure>
<p>这个时候就可以对网络中多个loss进行分步的训练了。</p>
<h2 id="冻结网络层"><a href="#冻结网络层" class="headerlink" title="冻结网络层"></a>冻结网络层</h2><blockquote>
<p>freeze</p>
</blockquote>
<h3 id="eval"><a href="#eval" class="headerlink" title="eval"></a>eval</h3><p>会固定网络中的BN、dropout等在训练和测试的时候行为不一致的模块。</p>
<h3 id="冻结指定的层"><a href="#冻结指定的层" class="headerlink" title="冻结指定的层"></a>冻结指定的层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#需要先对指定的层设置grad_fn=false</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line"><span class="keyword">if</span> param[<span class="number">0</span>] <span class="keyword">in</span> need_frozen_list:</span><br><span class="line">param[<span class="number">1</span>].requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这种方法需要注意的是层名一定要和model中一致，model经过.cuda后往往所用层会添加module.的前缀，会导致后面的冻结无效。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后需要改正optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, model.parameters()), lr=args.lr,momentum=args.momentum, weight_decay=args.weight_decay)</span><br></pre></td></tr></table></figure>

<h3 id="named-parameters实现"><a href="#named-parameters实现" class="headerlink" title="named_parameters实现"></a>named_parameters实现</h3><p>可以通过named_parameters来找到指定的层进行固定</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># named_parameters以键值的形式存储</span></span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> net.named_parameters():</span><br><span class="line"><span class="keyword">if</span> m[<span class="number">0</span>].find(<span class="string">&#x27;CenterNessNet&#x27;</span>) != -<span class="number">1</span>:</span><br><span class="line">m[<span class="number">1</span>].requires_grad = <span class="literal">True</span> <span class="comment">#只要CenterNetssNet部分的值进行训练</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">m[<span class="number">1</span>].requries_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<h3 id="只冻结BN"><a href="#只冻结BN" class="headerlink" title="只冻结BN"></a>只冻结BN</h3><h4 id="问题-1"><a href="#问题-1" class="headerlink" title="问题"></a>问题</h4><p>由于bn城的参数不光与其待训练的参数有关，还与runing_mean和runing_var两个参数有关，而这2个参数是通过统计得到的，如果直接使用常规的冻结操作：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> self.<span class="property">detection_net</span>:</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> p.<span class="title function_">parameters</span>():</span><br><span class="line">param.<span class="property">requires_grad</span> = <span class="title class_">False</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>是无法有效冻结bn操作的</p>
</blockquote>
<h4 id="solution"><a href="#solution" class="headerlink" title="solution"></a>solution</h4><p>这里通过apply函数和class属性来实现</p>
<h5 id="class属性"><a href="#class属性" class="headerlink" title="class属性"></a><strong>class属性</strong></h5><p>可以使用模型的class属性中的name变量来查看当前层的类型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">l = [conv,bn]</span><br><span class="line">basename = l.__class__.__name__</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> basename:</span><br><span class="line"><span class="built_in">print</span>(m)</span><br><span class="line"></span><br><span class="line"><span class="comment"># conv</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bn</span></span><br></pre></td></tr></table></figure>
<p>⚠️ 问题通过以上方式是无法便利出Sequential封装的，所以这个时候就需要使用apply</p>
<h5 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h5><p>为当前的元组（或list）的每个元素应用所指定的函数。</p>
<h5 id="最终实现"><a href="#最终实现" class="headerlink" title="最终实现"></a>最终实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fix_bn</span>(<span class="params">m</span>):</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个操作只能获得名称，所以不适合考虑前缀的操作，只能找BN用</span></span><br><span class="line">classname = m.__class__.__name__</span><br><span class="line"><span class="keyword">if</span> classname.find(<span class="string">&#x27;BatchNorm&#x27;</span>) != -<span class="number">1</span>:</span><br><span class="line">m.<span class="built_in">eval</span>()</span><br><span class="line">model = models.resnet50(pretrained=<span class="literal">True</span>)</span><br><span class="line">model.cuda()</span><br><span class="line">model.train()</span><br><span class="line">model.apply(fix_bn) <span class="comment"># fix batchnorm</span></span><br></pre></td></tr></table></figure>
<p>通过以上方式就可以很好的固定BN操作了。</p>
<h4 id="实现二"><a href="#实现二" class="headerlink" title="实现二"></a>实现二</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">fix_bn</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="function">for layer in self.modules():</span></span><br><span class="line"><span class="function">if isinstanace(layer, nn.BatchNorm2d):</span></span><br><span class="line"><span class="function">layer.eval()</span></span><br></pre></td></tr></table></figure>

<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><p>Pytorch的数据读取主要由3个类完成：Dataset、Samper、DataLoader。Dataset用来读取单张样本、Samper对数据集中的样本进行采样、DataLoader根据采样顺序读取一个Batch的数据。为了更加直观，这里反向解释。</p>
<h2 id="dataloader"><a href="#dataloader" class="headerlink" title="dataloader"></a>dataloader</h2><blockquote>
<p>pytorch&#x2F;torch&#x2F;utils&#x2F;data&#x2F;dataloader.py</p>
</blockquote>
<p>调用了sampler函数进行采样工作<br>定义了由多少个进程(线程)来处理数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoader</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line"><span class="comment"># sampler：生成一系列的index</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_sampler:将sampler生成的indices打包分组，得到一个又一个batch的index</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=default_collate,</span></span><br><span class="line"><span class="params">pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">worker_init_fn=<span class="literal">None</span></span>)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于DataLoader主要是为了迭代读取，所以只需要定义next和iter，iter用来构建迭代对象</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__next__</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_worker简单理解就是并行读取数据，这里为了简单简化了</span></span><br><span class="line"><span class="keyword">if</span> self.num_workers == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取index的方式有多种，有按顺序的，也有乱序的，所以这个工作需要Sampler完成，</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataLoader和Sampler在这里产生关系</span></span><br><span class="line">indices = <span class="built_in">next</span>(self.sampler_iter) <span class="comment"># Sampler，获取采样中指定数量的图片，读取数据就只需要对应的index即可</span></span><br><span class="line">batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]) <span class="comment"># Dataset， 拿到了index就可以使用Dataset进行样本读取</span></span><br><span class="line"><span class="keyword">if</span> self.pin_memory: <span class="comment"># True时，Pytorch会采取一系列操作把数据拷贝到GPU，总之就是为了加速。</span></span><br><span class="line">batch = _utils.pin_memory.pin_memory_batch(batch)</span><br><span class="line"><span class="keyword">return</span> batch</span><br></pre></td></tr></table></figure>
<p>((6258f20b-fc4d-4e0e-8fd4-9cbd2ddfb82b))的作用就是将一个batch的数据进行合并操作。</p>
<h2 id="Sampler"><a href="#Sampler" class="headerlink" title="Sampler"></a>Sampler</h2><blockquote>
<p>Sampler返回的是样本的index（位置信息）</p>
</blockquote>
<p>Pytorch已经实现的Sampler有如下几种：<br>[[SequentialSampler]]<br>[[RandomSampler]]<br>[[WeightedRandomSampler]]<br>[[SubsetRandomSampler]]</p>
<blockquote>
<p>Sampler是对数据集的采样操作，而<a target="_blank" rel="noopener" href="https://www.notion.so/BatchSampler-1ee7eccb9ec44ff784eae15a3caa1a71">BatchSampler</a>将Sampler生成的index按照指定的batch size分组，返回的结果是以组为单位的index list。</p>
</blockquote>
<p>⚠️ BatchSampler是实现Batch的关键！BatchSampler与其他Sampler的主要区别是它需要将Sampler作为参数进行打包，进而每次迭代返回以batch size为大小的index列表。也就是说在后面的读取数据过程中使用的都是batch sampler。</p>
<h3 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 所有Sampler都需要继承该父类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Sampler</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="string">r&quot;&quot;&quot;Base class for all Samplers.</span></span><br><span class="line"><span class="string">Every Sampler subclass has to provide an :meth:`__iter__` method, providing a</span></span><br><span class="line"><span class="string">way to iterate over indices of dataset elements, and a :meth:`__len__` method</span></span><br><span class="line"><span class="string">that returns the length of the returned iterators.</span></span><br><span class="line"><span class="string">.. note:: The :meth:`__len__` method isn&#x27;t strictly required by</span></span><br><span class="line"><span class="string">:class:`~torch.utils.data.DataLoader`, but is expected in any</span></span><br><span class="line"><span class="string">calculation involving the length of a :class:`~torch.utils.data.DataLoader`.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_source</span>):</span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同的Sampler方法主要是在这里构建不同的可迭代对象。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 例如SequentialSampler返回的是iter(range(len(self.data_source)))</span></span><br><span class="line"><span class="keyword">raise</span> NotImplementedError</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">len</span>(self.data_source)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>通常会使用BatchSampler来进行Batch化。</p>
</blockquote>
<blockquote>
</blockquote>
<p>这里以SequentialSampler采样为例，进行BatchSampler：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">in</span> : <span class="built_in">list</span>(BatchSampler(SequentialSampler(<span class="built_in">range</span>(<span class="number">10</span>)), batch_size=<span class="number">3</span>, drop_last=<span class="literal">False</span>))</span><br><span class="line">&gt;&gt;&gt;out: [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>DataLoader的部分初始化参数之间存在互斥关系</p>
<ol>
<li>如果你自定义了batch_sampler,那么这些参数都必须使用默认值：batch_size, shuffle,sampler,drop_last.</li>
<li>如果你自定义了sampler，那么shuffle需要设置为False</li>
<li>如果sampler和batch_sampler都为None,那么batch_sampler使用Pytorch已经实现好的BatchSampler,而sampler分两种情况：</li>
<li>若shuffle&#x3D;True,则sampler&#x3D;RandomSampler(dataset)</li>
<li>若shuffle&#x3D;False,则sampler&#x3D;SequentialSampler(dataset)</li>
</ol>
<h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于Sampler会对数据进行采样，所以这里就需要按指定位置来读取样本，所以就需要getitem，而不是next</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line"><span class="keyword">return</span> ... <span class="comment"># 规定了如何读取数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="keyword">return</span> ...</span><br></pre></td></tr></table></figure>
<blockquote>
<p>getitem的主要作用是能让该类可以像list一样通过索引值对数据进行访问。</p>
</blockquote>
<h3 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h3><p>假如你定义好了一个dataset，那么你可以直接通过dataset[0]来访问第一个数据。在此之前我一直没弄清楚__getitem__是什么作用，所以一直不知道该怎么进入到这个函数进行调试。现在如果你想对__getitem__方法进行调试，你可以写一个for循环遍历dataset来进行调试了，而不用构建dataloader等一大堆东西了，建议学会使用ipdb这个库，非常实用！！！以后有时间再写一篇ipdb的使用教程。另外，其实我们通过最前面的Dataloader的__next__函数可以看到DataLoader对数据的读取其实就是用了for循环来遍历数据。</p>
<h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><blockquote>
<p>其实就是具备GPU加速功能的numpy</p>
</blockquote>
<h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><p>Variable是对Tensor的封装，从Pytorch1.0开始，Variable就已经被取消了，其实就是将Variable的功能集成到了Tensor里。这里介绍的Variable，相当于就是Tensor对numpy的扩充部分。<br><strong>Variable三元素:</strong></p>
<ol>
<li>Tensor: 数据部分</li>
<li>grad:Tensor的梯度</li>
<li>grad_fn:  以何种方式得到这种梯度<br>计算图就是建立在Variable这个数据结构之上的，方便神经网络的构建。 Pytorch默认做完一次自动求导之后就会丢弃计算图，所以在没有使用retain_graph的时候，只能使用一次backward，第二次使用就会报错(因为grad_fn其实就不存在了，没有计算图了怎么还会有如何计算)。</li>
</ol>
<h3 id="Parameter"><a href="#Parameter" class="headerlink" title="Parameter"></a>Parameter</h3><p>这里以最直观的梯度下降更新网络参数为例，展示参数更新过程:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w.data = w.data lr * w.grad.data  <span class="comment"># lr 是学习率</span></span><br></pre></td></tr></table></figure>
<p><strong>Variable主要是为了解决计算图构建问题，但是它的性质导致其无法作为网络的参数:</strong></p>
<ol>
<li>Variable默认是不需要计算梯度的，需要手动设置参数requires_grad&#x3D;True</li>
<li>Pytorch默认只计算一次计算图就丢弃，这就导致Variable容易被丢弃，在backward的时候还需要手动设置参数w.backward(retrain_grad&#x3D;True)来保持计算图。</li>
<li>网络中若是有100个参数，都要手写更新代码吗？1000个呢？10000个呢</li>
</ol>
<p><strong>Pytorch引入nn.Parameter类型和optimizer机制来解决。创建parameter的方式:</strong></p>
<ol>
<li>我们可以直接将**模型的成员变量(**self. )通过nn.Parameter() 创建，会自动注册到parameters中，可以通过model.parameters() 返回，并且这样创建的参数会自动保存到OrderDict中去;</li>
<li>通过nn.Parameter()创建普通的Parameter对象，不作为模型的成员变量，然后将Parameter对象通过register_parameter()进行注册，这个时候也会保存到到网络的parameters()对象里。<br>Parameter是Variable的子类，默认是需要求梯度的。网络net中的所有parameter变量都可以通过net.parameters()来进行访问，这样的好处就是不需要手动为每个参数都写更新代码，只需要使用parameter就行了。 只需将网络中所有需要训练更新的参数定义为Parameter类型，再佐以optimizer，就能够完成所有参数的更新了。</li>
</ol>
<h4 id="实现-2"><a href="#实现-2" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, a, b, ...</span>):</span><br><span class="line"><span class="built_in">super</span>(net, self).__init__()</span><br><span class="line">self...   <span class="comment">#  parameters</span></span><br><span class="line">self...    <span class="comment"># layers</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self</span>):</span><br><span class="line">x = ...</span><br><span class="line">x = ...    <span class="comment"># 数据流</span></span><br><span class="line"><span class="keyword">return</span> x</span><br><span class="line">net = Net(a, b, ...)</span><br><span class="line">net.train()</span><br><span class="line">...</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">1e-1</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在每一次batch操作中，只需要在每次loss.backward()之后调用optimizer.step()就可以完成参数的更新。</p>
</blockquote>
<h4 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h4><p>需要注册，通过注册才能被网络跟踪，可以通过state_dict读取，保存到OrderDict里。</p>
<h3 id="named-parameters"><a href="#named-parameters" class="headerlink" title="named_parameters()"></a>named_parameters()</h3><p>会同时给出网络层的名字和参数的字典，而parameters()只会给出参数(权重)。</p>
<h3 id="buffer"><a href="#buffer" class="headerlink" title="buffer"></a>buffer</h3><p>与parameter对应，parameter是网络中需要进行更新的参数，而buffer是不需要更新的参数。<br>buffer与parameter的异同:</p>
<ol>
<li>都需要通过等级才能通过model.state_dict()查看，parameter通过register_parameter登记，buffer通过register_buffer进行登记</li>
<li>buffer其实就是requires_grad&#x3D;False，但是为了更方便总体使用，所以进行不同的封装。</li>
</ol>
<h3 id="归并运算"><a href="#归并运算" class="headerlink" title="归并运算"></a>归并运算</h3><p>与numpy对应，只是参数的名字不一样，在numpy中用axis指定的维度在tensor中使用dim表示。<br>dim：指定的维度会被聚合<br>keepdim：默认False。表示不保留聚合的维度，True会保留维度为1。</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>mean&#x2F;sum&#x2F;median&#x2F;mode</td>
<td>均值&#x2F;和&#x2F;中位数&#x2F;众数</td>
</tr>
<tr>
<td>norm&#x2F;dist</td>
<td>范数&#x2F;距离</td>
</tr>
<tr>
<td>std&#x2F;var</td>
<td>标准差&#x2F;方差</td>
</tr>
<tr>
<td>cumsum&#x2F;cumprod</td>
<td>累加&#x2F;累乘</td>
</tr>
<tr>
<td>numel</td>
<td>返回元素数目</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="DataParallel"><a href="#DataParallel" class="headerlink" title="DataParallel"></a>DataParallel</h2><p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1650003096867_0.png" alt="image.png"><br>上图中并没有将opitmizer封装到DataParallel里，这是由于每次前向传播的时候都会分发模型，用不着反向传播时将梯度loss分发到各个GPU单独计算梯度，在进行合并的操作。<br>可以就在主GPU上根据总loss更新模型的梯度，并且不用同步其他GPU上的模型，因为前向传播的时候会分发模型。</p>
<blockquote>
<p>DataParallel是没有完成对GPU的调用的，需要手动cuda上去。</p>
</blockquote>
<h3 id="基础示例"><a href="#基础示例" class="headerlink" title="基础示例"></a>基础示例</h3><p>在进行多ＧＰＵ训练的场景，PyTorch通常使用nn.DataParallel来包装网络模型，它会将模型在每张卡上都复制一份，从而实现并行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device_ids = [<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">net = torch.nn.DataParallel(net, device_ids = device_ids)</span><br></pre></td></tr></table></figure>

<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>DataParallel会导致其中一块卡的显存占用高于其他块。<br>容易导致负载不均衡，这个时候就需要将loss分配的到多个GPU上进行计算，然后使用loss.mean or loss.sum等方式进行归并。这样主GPU只会多一点占用。</p>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.nn.DataParallel(module, device_ids=<span class="literal">None</span>, output_device=<span class="literal">None</span>, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>module:需要并行计算的模型<br>device_ids:可用ＧＰＵ<br>output_device:当调用nn.DataParallel的时候，只是在input数据是并行的，但是output loss却不是这样的，每次都会在第一块GPU相加计算。这里默认是device_ids[0]<br>dim:对数据进行分割的维度<br>由于DataParallel的设计，导致在运行DataParallel模块之前，并行化模块必须在device_ids[0]上具有其参数和缓冲区,如果不想占用多余的卡就需要设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">&quot;CUDA_DEVICE_ORDER&quot;</span>] = <span class="string">&quot;PCI_BUS_ID&quot;</span></span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;2, 3&quot;</span></span><br></pre></td></tr></table></figure>
<p>使得网络在识别的时候，只识别这２张卡，２卡就会成为第一张卡来存储DataParallel的缓冲区。</p>
<h3 id="DataParallel-CODE"><a href="#DataParallel-CODE" class="headerlink" title="DataParallel CODE"></a>DataParallel CODE</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, *<span class="built_in">input</span>, **kwargs</span>):</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> self.device_ids:</span><br><span class="line"><span class="keyword">return</span> self.module(*inputs, **kwargs)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> chain(self.module.parameters(), self.module.buffers()):</span><br><span class="line"><span class="keyword">if</span> t.device != self.src_device_obj:</span><br><span class="line"><span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;模型必须在设备&#123;&#125;(device_ids[0])上有它</span></span><br><span class="line"><span class="string">的参数和缓冲区，但是找到的设备是&#123;&#125;&#x27;</span>.formot(self.src_device_obj, t.device))</span><br><span class="line">inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(self.device_ids) == <span class="number">1</span>:</span><br><span class="line"><span class="keyword">return</span> self.module(*inputs[<span class="number">0</span>], **kwargs[<span class="number">0</span>])</span><br><span class="line">replicas = self.replicate(self.module, self.device_ids[:<span class="built_in">len</span>(inputs)])</span><br><span class="line">outputs = self.parallel_apply(replicas, inputs, kwargs)</span><br><span class="line"><span class="keyword">return</span> self.gather(outputs, self.output_device)</span><br></pre></td></tr></table></figure>
<p>src_device_obj:第一张卡，默认为device_ids[0]<br><strong>其中涉及到了４个方法：</strong><br>scatter:将输入数据及参数均分到每张卡上<br>replicate:将模型复制一份(注意：卡上必须有scatter分割的数据存在！)<br>parallel_apply:每张卡并行计算结果，这里会调用被包装的具体模型的前向反馈操作<br>gather:将每张卡的计算结果统一汇聚到主卡</p>
<h1 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h1><h2 id="gather"><a href="#gather" class="headerlink" title="gather"></a>gather</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.gather(<span class="built_in">input</span>, dim, index, out=NOne)-&gt;Tensor</span><br></pre></td></tr></table></figure>
<p>沿着给定的轴dim，对输入索引张量index指定位置的值进行聚合。其实就是只有指定的维度上使用到了index上的值。<br>对一个三维张量，输出可以定义为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#空格是为了好看一点</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[index[i][j][k]]  [j]  [k] <span class="comment"># dim==0</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i]  [index[i][j][k]]  [k] <span class="comment"># dim==1</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i]  [j]  [index[i][j][k]] <span class="comment"># dim==2</span></span><br></pre></td></tr></table></figure>
<p>可以看出index的形状需要和out相同。<br>同时还要注意：<strong>除了指定的维度的长度外，其他维度的长度必须相同才能执行，否则会报错，同时i j k是按照index能够取得的范围来定的。</strong></p>
<h2 id="collate-fn"><a href="#collate-fn" class="headerlink" title="collate_fn"></a>collate_fn</h2><p>id:: 6258f20b-fc4d-4e0e-8fd4-9cbd2ddfb82b<br>默认的collate_fn是将img和label分别合并成imgs和labels，所以如果你的__getitem__方法只是返回 img, label,那么你可以使用默认的collate_fn方法，但是如果你每次读取的数据有img, box, label等等，那么你就需要自定义collate_fn来将对应的数据合并成一个batch数据，这样方便后续的训练步骤。</p>
<h3 id="拼接"><a href="#拼接" class="headerlink" title="拼接"></a>拼接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch = self.collate_fn([self.dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices])</span><br></pre></td></tr></table></figure>
<p>以上式为例：</p>
<ol>
<li>indices: 表示每一个iteration，sampler返回的indices，即一个batch size大小的索引列表</li>
<li>self.dataset[i]: 前面已经介绍了，这里就是对第i个数据进行读取操作，一般来说self.dataset[i]&#x3D;(img, label)<blockquote>
<p>拼接应该就是通过这个函数完成的，但是indices的获取应该还是BatchSampler的作用，所以它们是不冲突的，BatchSampler获取一个batch的index之后，在这里进行样本对象的拼接。</p>
</blockquote>
</li>
</ol>
<h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p>因为在训练时的数据维度一般都是 (batch_size, c, h, w)，而在测试时只输入一张图片，所以需要扩展维度。</p>
<h1 id="view"><a href="#view" class="headerlink" title="view"></a>view</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">image = cv2.imread(img_path)</span><br><span class="line">image = torch.tensor(image)</span><br><span class="line"><span class="built_in">print</span>(image.size())</span><br><span class="line">img = image.view(<span class="number">1</span>, *image.size())</span><br><span class="line"><span class="built_in">print</span>(img.size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([h, w, c])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([1, h, w, c])</span></span><br></pre></td></tr></table></figure>

<h1 id="numpy-newaxis"><a href="#numpy-newaxis" class="headerlink" title="numpy.newaxis"></a>numpy.newaxis</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">image = cv2.imread(img_path)</span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br><span class="line">img = image[np.newaxis, :, :, :]</span><br><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (h, w, c)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># (1, h, w, c)</span></span><br></pre></td></tr></table></figure>

<h1 id="unsqueeze"><a href="#unsqueeze" class="headerlink" title="unsqueeze"></a>unsqueeze</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">image = cv2.imread(img_path)</span><br><span class="line">image = torch.tensor(image)</span><br><span class="line"><span class="built_in">print</span>(image.size())</span><br><span class="line">img = image.unsqueeze(dim=<span class="number">0</span>)  </span><br><span class="line"><span class="built_in">print</span>(img.size())</span><br><span class="line">img = img.squeeze(dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(img.size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([(h, w, c)])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([1, h, w, c])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([h, w, c])</span></span><br></pre></td></tr></table></figure>

<h2 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h2><p>内置了多个模型和其权重：<br>AlexNet<br>VGG<br>ResNet<br>SqueezeNet<br>DenseNet</p>
<h2 id="查看部分样本信息"><a href="#查看部分样本信息" class="headerlink" title="查看部分样本信息"></a>查看部分样本信息</h2><h3 id="torch-index-select"><a href="#torch-index-select" class="headerlink" title="torch.index_select()"></a>torch.index_select()</h3><p>用于索引给定张量中某一个维度中元素的方法，其API手册如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select(<span class="built_in">input</span>, dim, index, out=<span class="literal">None</span>) → Tensor</span><br><span class="line">Parameters: </span><br><span class="line"><span class="built_in">input</span> (Tensor) – 输入张量，需要被索引的张量</span><br><span class="line">dim (<span class="built_in">int</span>) – 在某个维度被索引</span><br><span class="line">index (LongTensor) – 一维张量，用于提供索引信息</span><br><span class="line">out (Tensor, optional) – 输出张量，可以不填</span><br></pre></td></tr></table></figure>
<p>其作用很简单，比如我现在的输入张量为1000 * 10的尺寸大小，其中1000为样本数量，10为特征数目，如果我现在需要<strong>指定的某些样本</strong>，比如第1-100,300-400等等样本，我可以用一个<strong>index进行索引</strong>，然后应用torch.index_select()就可以索引了，例子如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.1427</span>,  <span class="number">0.0231</span>, -<span class="number">0.5414</span>, -<span class="number">1.0009</span>],</span><br><span class="line">[-<span class="number">0.4664</span>,  <span class="number">0.2647</span>, -<span class="number">0.1228</span>, -<span class="number">1.1068</span>],</span><br><span class="line">[-<span class="number">1.1734</span>, -<span class="number">0.6571</span>,  <span class="number">0.7230</span>, -<span class="number">0.6004</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">0</span>, indices) <span class="comment"># 按行索引</span></span><br><span class="line">tensor([[ <span class="number">0.1427</span>,  <span class="number">0.0231</span>, -<span class="number">0.5414</span>, -<span class="number">1.0009</span>],</span><br><span class="line">[-<span class="number">1.1734</span>, -<span class="number">0.6571</span>,  <span class="number">0.7230</span>, -<span class="number">0.6004</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">1</span>, indices) <span class="comment"># 按列索引</span></span><br><span class="line">tensor([[ <span class="number">0.1427</span>, -<span class="number">0.5414</span>],</span><br><span class="line">[-<span class="number">0.4664</span>, -<span class="number">0.1228</span>],</span><br><span class="line">[-<span class="number">1.1734</span>,  <span class="number">0.7230</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：pytorch似乎在使用GPU的情况下，不检查index是否会越界，因此如果你的index越界了，但是报错的地方可能不在使用index_select()的地方，而是在后续的代码中，这个似乎就需要留意下你的index了。同时，index是一个LongTensor，这个也是要留意的。</p>
</blockquote>
<h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><h3 id="ops"><a href="#ops" class="headerlink" title="ops"></a>ops</h3><table>
<thead>
<tr>
<th>函数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>batched_nms</td>
<td>根据每个类别进行过滤，只对同一种类别进行计算IOU和阈值过滤</td>
</tr>
<tr>
<td>nms</td>
<td>不区分类别对所有bbox进行过滤。如果有不同类别的bbox重叠的话会导致被过滤掉并不会分开计算。</td>
</tr>
</tbody></table>
<h2 id="ray"><a href="#ray" class="headerlink" title="ray"></a>ray</h2><blockquote>
<p>基于 Python 的分布式计算框架，采用动态图计算模型。使用起来很方便可通过装饰器的方式，仅需修改极少的的代码，让原本运行在单机的 Python 代码轻松实现分布式计算。目前多用于机器学习方面</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ray</span><br><span class="line">ray.<span class="built_in">init</span>()</span><br><span class="line">@ray.<span class="function">remote</span></span><br><span class="line"><span class="function">def <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line"><span class="function">return x * x</span></span><br><span class="line"><span class="function">futures =</span> [f.<span class="built_in">remote</span>(i) <span class="function"><span class="keyword">for</span> i in <span class="title">range</span><span class="params">(<span class="number">4</span>)</span>]</span></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(ray.get(futures))</span></span></span><br></pre></td></tr></table></figure>

<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="torch-index-select-input-dim-index"><a href="#torch-index-select-input-dim-index" class="headerlink" title="torch.index_select(input,dim,index)"></a>torch.index_select(input,dim,index)</h3><p>在维度dim上，按index索引数据<br>返回值：依index索引数据拼接成张量</p>
<h3 id="torch-masked-select-input-mask"><a href="#torch-masked-select-input-mask" class="headerlink" title="torch.masked_select(input,mask)"></a>torch.masked_select(input,mask)</h3><p>功能：按mask中的True进行索引<br>返回值：一维张量</p>
<h2 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h2><blockquote>
<p>基础函数，不带自动反向求导。可以用来生成Module模型(带自动求导)。</p>
</blockquote>
<h3 id="ctx"><a href="#ctx" class="headerlink" title="ctx"></a>ctx</h3><p>在function里会封装一个ctx函数，用来传递前向forward的结果到反向backwark进行梯度求导。</p>
<h3 id="以ChannNorm为例"><a href="#以ChannNorm为例" class="headerlink" title="以ChannNorm为例"></a>以ChannNorm为例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function, Variable</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.module <span class="keyword">import</span> Module</span><br><span class="line"><span class="keyword">import</span> channelnorm_cuda <span class="comment">#cuda编写的函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelNormFunction</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, input1, norm_deg=<span class="number">2</span></span>):</span><br><span class="line"></span><br><span class="line"><span class="comment">#需要保证输入是连续的</span></span><br><span class="line"><span class="keyword">assert</span> input1.is_contiguous()</span><br><span class="line">b, _, h, w = input1.size()</span><br><span class="line">output = input1.new(b,<span class="number">1</span>,h,w).zero_()</span><br><span class="line">channel_cuda.forward(input1, output, norm_deg)</span><br><span class="line"></span><br><span class="line"><span class="comment">#可以看出针对参数变量使用save_for_backward来进行存储</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 应该是为了保存参数的特性，如类型</span></span><br><span class="line">ctx.save_for_backward(input1, output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对不需要求导的对象则使用成员变量的形式存储</span></span><br><span class="line">ctx.norm_deg = norm_deg</span><br><span class="line"><span class="keyword">return</span> output</span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line"></span><br><span class="line"><span class="comment"># saved_tensors恢复ctx.save_for_backward存储的对象</span></span><br><span class="line">input1, output = ctx.saved_tensors</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将取出的对象使用Variable封装</span></span><br><span class="line">grad_input1 = Variable(input1.new(input1.size()).zero_())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用backward函数进行反向求导</span></span><br><span class="line">channelnorm_cuda.backward(input1, output, grad_output.data, grad_input1.data, ctx.norm_deg)</span><br><span class="line"><span class="keyword">return</span> grad_input1, <span class="literal">None</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelNorm</span>(<span class="title class_ inherited__">Module</span>):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, norm_deg=<span class="number">2</span></span>):</span><br><span class="line"><span class="built_in">super</span>(ChannelNorm, self).__init__()</span><br><span class="line">self.norm_deg = norm_deg</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Module来封装Function，实现自动求导。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input1</span>):</span><br><span class="line"><span class="keyword">return</span> |ChannelNormFunction.apply(input1, self.norm_deg)</span><br></pre></td></tr></table></figure>

<h2 id="hook"><a href="#hook" class="headerlink" title="hook"></a>hook</h2><blockquote>
<p>Hook 是 PyTorch 中一个十分有用的特性。利用它，我们可以不必改变网络输入输出的结构，方便地获取、改变网络中间层变量的值和梯度。</p>
</blockquote>
<h3 id="why"><a href="#why" class="headerlink" title="why"></a>why</h3><p>pytorch在每一次运算结束后，会将中间变量释放，以节省内存空间，这些会被释放的变量包括非叶子张量的梯度，中间层的特征图等。但有时候，我们想可视化中间层的特征图，又不能改动模型主体代码，该怎么办呢？这时候就要用到hook了。</p>
<blockquote>
<p>可视化中间结果，如梯度(反向传播之后中间节点的梯度会被清空)</p>
</blockquote>
<p>可视化参数，感觉有了tensorboardx之类的工具了，这个要来干嘛莫。<br>当前的kook主要分为2类：</p>
<ol>
<li>针对tensor</li>
<li>针对nn.Moudle</li>
</ol>
<h3 id="Hook-for-Tensors"><a href="#Hook-for-Tensors" class="headerlink" title="Hook for Tensors"></a>Hook for Tensors</h3><blockquote>
<p>torch.Tensor.register_hook (Python method, in torch.Tensor)</p>
</blockquote>
<p><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/image_1650003370050_0.png" alt="image.png"></p>
<h4 id="存在意义"><a href="#存在意义" class="headerlink" title="存在意义"></a>存在意义</h4><p>在 PyTorch 的计算图（computation graph）中，只有叶子结点（leaf nodes）的变量会保留梯度。而所有中间变量的梯度只被用于反向传播，一旦完成反向传播，中间变量的梯度就将自动释放，从而节约内存。</p>
<blockquote>
<p>但是可以通过retain_grad进行梯度的持久化。保留中间变量。但是这种方法会增加内存占用。所以就有了hook。</p>
</blockquote>
<blockquote>
</blockquote>
<p>对于中间变量z，hook的使用方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z.register_hook(hook_fn)</span><br></pre></td></tr></table></figure>
<p>其中hook_fn是一个用户自定义的函数，其签名为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#后面的表示返回值</span></span><br><span class="line">hook_fn(grad)-&gt;Tensor <span class="keyword">or</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hook_fn</span>(<span class="params">grad</span>):</span><br><span class="line"><span class="built_in">print</span>(grad)</span><br><span class="line"></span><br><span class="line"><span class="comment">#这样z就可以通过z.register_hook(hook_fn)来捕获值了</span></span><br></pre></td></tr></table></figure>
<p>结果和上文中 z.retain_grad()方法得到的 z 的偏导一致。</p>
<blockquote>
<p>hook会改变变量的值，同时一个变量可以绑定多个hook_fn，反向传播时，它们按绑定顺序依次执行。</p>
</blockquote>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><h5 id="修改中间结果的梯度"><a href="#修改中间结果的梯度" class="headerlink" title="修改中间结果的梯度"></a>修改中间结果的梯度</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grad_hook</span>(<span class="params">grad</span>):</span><br><span class="line">grad *= <span class="number">2</span></span><br><span class="line">x = torch.tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.<span class="built_in">pow</span>(x, <span class="number">2</span>)</span><br><span class="line">z = torch.mean(y)</span><br><span class="line">h = x.register_hook(grad_hook)</span><br><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">h.remove()    <span class="comment"># removes the hook</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>])</span><br></pre></td></tr></table></figure>
<p>原x的梯度为tensor([1., 1., 1., 1.])，经grad_hook操作后，梯度为tensor([2., 2., 2., 2.])。</p>
<h5 id="查看中间节点的梯度"><a href="#查看中间节点的梯度" class="headerlink" title="查看中间节点的梯度"></a>查看中间节点的梯度</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">y_grad= <span class="built_in">list</span>()</span><br><span class="line">defgrad_hook(grad):</span><br><span class="line">y_grad.append(grad)</span><br><span class="line">x= torch.tensor([<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y= torch.<span class="built_in">pow</span>(x, <span class="number">2</span>)</span><br><span class="line">z= torch.mean(y)</span><br><span class="line">h= y.register_hook(grad_hook)</span><br><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y.grad: &quot;</span>, y.grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_grad[0]: &quot;</span>, y_grad[<span class="number">0</span>])</span><br><span class="line">h.remove()<span class="comment"># removes the hook&gt;&gt;&gt; (&#x27;y.grad: &#x27;, None)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="string">&#x27;y_grad[0]: &#x27;</span>, tensor([<span class="number">0.2500</span>, <span class="number">0.2500</span>, <span class="number">0.2500</span>, <span class="number">0.2500</span>]))</span><br></pre></td></tr></table></figure>
<p>可以看到当z.backward()结束后，张量y中的grad为None，因为y是非叶子节点张量，在梯度反传结束之后，被释放。 在对张量y的hook函数（grad_hook）中，将y的梯度保存到了y_grad列表中，因此可以在z.backward()结束后，仍旧可以在y_grad[0]中读到y的梯度为tensor([0.2500, 0.2500, 0.2500, 0.2500])</p>
<h3 id="Hook-for-Modules"><a href="#Hook-for-Modules" class="headerlink" title="Hook for Modules"></a>Hook for Modules</h3><h4 id="why-1"><a href="#why-1" class="headerlink" title="why"></a>why</h4><blockquote>
<p>但是对于模型而言，对于夹在网络中间的模块，我们不但很难得知它输入&#x2F;输出的梯度，甚至连它输入输出的数值都无法获得除非设计网络时，在 forward 函数的返回值中包含中间 module 的输出，或者用很麻烦的办法，把网络按照 module 的名称拆分再组合，让中间层提取的 feature 暴露出来。</p>
</blockquote>
<h2 id="how"><a href="#how" class="headerlink" title="how"></a>how</h2><p>下面剖析一下<strong>module</strong>是怎么样<strong>调用hook</strong>函数的呢？</p>
<ol>
<li>output &#x3D; net(fake_img) net是一个module类，对module执行 module(input)是会调用module.call</li>
<li>module.<strong>call</strong> 在module.call中执行流程如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, *<span class="built_in">input</span>, **kwargs</span>):</span><br><span class="line"><span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_pre_hooks.values():</span><br><span class="line">hook(self, <span class="built_in">input</span>)</span><br><span class="line"><span class="keyword">if</span> torch._C._get_tracing_state():</span><br><span class="line">result = self._slow_forward(*<span class="built_in">input</span>, **kwargs)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">result = self.forward(*<span class="built_in">input</span>, **kwargs)</span><br><span class="line"><span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_hooks.values(): <span class="comment"># 前向hook不能有返回值</span></span><br><span class="line">hook_result = hook(self, <span class="built_in">input</span>, result)</span><br><span class="line"><span class="keyword">if</span> hook_result <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"><span class="keyword">raise</span> RuntimeError(</span><br><span class="line"><span class="string">&quot;forward hooks should never return any values, but &#x27;&#123;&#125;&#x27;&quot;</span></span><br><span class="line"><span class="string">&quot;didn&#x27;t return None&quot;</span>.<span class="built_in">format</span>(hook))</span><br><span class="line">...省略</span><br></pre></td></tr></table></figure>
可以看出hook就定义在模型执行内部。</li>
</ol>
<h4 id="what"><a href="#what" class="headerlink" title="what"></a>what</h4><h5 id="torch-nn-Module-register-forward-hook→None"><a href="#torch-nn-Module-register-forward-hook→None" class="headerlink" title="torch.nn.Module.register_forward_hook→None"></a>torch.nn.Module.register_forward_hook→None</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nnas nn</span><br><span class="line">classNet(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">self.conv1= nn.Conv2d(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">self.pool1= nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">x= self.conv1(x)</span><br><span class="line">x= self.pool1(x)</span><br><span class="line"><span class="keyword">return</span> x</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">farward_hook</span>(<span class="params">module, data_input, data_output</span>): <span class="comment"># 这里后两个变量是固定的，默认就对应了网络的输入和输出</span></span><br><span class="line">fmap_block.append(data_output)</span><br><span class="line">input_block.append(data_input)</span><br><span class="line"><span class="keyword">if</span> __name__== <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化网络net= Net()</span></span><br><span class="line">net.conv1.weight[<span class="number">0</span>].fill_(<span class="number">1</span>)</span><br><span class="line">net.conv1.weight[<span class="number">1</span>].fill_(<span class="number">2</span>)</span><br><span class="line">net.conv1.bias.data.zero_()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册hookfmap_block= list()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于hook不能有返回值，所以就需要在hook外部定义载体</span></span><br><span class="line">input_block= <span class="built_in">list</span>()</span><br><span class="line">net.conv1.register_forward_hook(farward_hook) <span class="comment"># 注册hook</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># inference</span></span><br><span class="line">fake_img= torch.ones((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))<span class="comment"># batch size * channel * H * W</span></span><br><span class="line">output= net(fake_img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察print(&quot;output shape: &#123;&#125;\noutput value: &#123;&#125;\n&quot;.format(output.shape, output))</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;feature maps shape: &#123;&#125;\noutput value: &#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(fmap_block[<span class="number">0</span>].shape, fmap_block[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input shape: &#123;&#125;\ninput value: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(input_block[<span class="number">0</span>][<span class="number">0</span>].shape, input_block[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<p>这里再conv1注册了hook，而conv1就是一个卷积模块(依旧是一个Moudle对象)，所以执行流程就是</p>
<ol>
<li>主体网络接受输入fake_img进行forward，并按call中的顺序执行(主体Module对象没有hook)</li>
<li>执行每个模块单元，并按call执行(这个时候就执行到了conv1，同样按Module call顺序执行，调用注册的hook)<br>⚠️ 在这里终于要执行我们注册的forward_hook函数了，就在hook_result &#x3D; hook(self, input, result)这里！ 看到这里我们需要注意两点：</li>
<li>hook_result &#x3D; hook(self, input, result)中的input和result不可以修改！ 这里的<strong>input对应forward_hook函数中的data_input，result对应forward_hook函数中的data_output，在conv1中，input就是该层的输入数据，result就是经过conv1层操作之后的输出特征图</strong>。虽然可以通过hook来对这些数据操作，但是不能修改这些值，否则会破坏模型的计算。</li>
<li>注册的hook函数是不能带返回值的，否则抛出异常，这个可以从代码中看到 if hook_result is not None: raise RuntimeError<br>总结一下调用流程： net(fake_img) –&gt; net.call : result &#x3D; self.forward(input, *kwargs) –&gt; net.forward: x &#x3D; self.conv1(x) –&gt; conv1.call:hook_result &#x3D; hook(self, input, result) hook就是我们注册的forward_hook函数了。</li>
</ol>
<h5 id="torch-nn-Module-register-forward-pre-hook"><a href="#torch-nn-Module-register-forward-pre-hook" class="headerlink" title="torch.nn.Module.register_forward_pre_hook"></a>torch.nn.Module.register_forward_pre_hook</h5><p>虽然也定义在Moudle call中，但是可以看到它的执行是在forward函数之前的</p>
<h5 id="torch-nn-Module-register-backward-hook→Tensor-or-None"><a href="#torch-nn-Module-register-backward-hook→Tensor-or-None" class="headerlink" title="torch.nn.Module.register_backward_hook→Tensor or None"></a>torch.nn.Module.register_backward_hook→Tensor or None</h5><p>Module反向传播中的hook,每次计算module的梯度后，自动调用hook函数。<br><strong>注意事项</strong>：当module有多个输入或输出时，grad_input和grad_output是一个tuple。<br><strong>应用场景举例</strong>：例如提取特征图的梯度 举例：采用register_backward_hook实现特征图梯度的提取，并结合Grad-CAM（基于类梯度的类激活图可视化）方法对卷积神经网络的学习模式进行可视化。</p>
<h1 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h1><h2 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h2><h3 id="终端条件"><a href="#终端条件" class="headerlink" title="终端条件"></a>终端条件</h3><p>在执行命令之前添加</p>
<blockquote>
<p>CUDA_LAUNCH_BLOCKING&#x3D;1</p>
</blockquote>
<p>由于cuda是异步操作，所以报错的地方不一定是真正有问题的地方，在无法定位错误位置的时候，可以使用这个执行参数使得cuda变为同步的，这样就方便定位错误了。</p>
<h3 id="查看函数功能"><a href="#查看函数功能" class="headerlink" title="查看函数功能"></a>查看函数功能</h3><p>可以在python终端下使用：</p>
<blockquote>
<p>help(torch.gather)</p>
</blockquote>
<p>来实现函数的使用说明。</p>
<h3 id="指定GPU"><a href="#指定GPU" class="headerlink" title="指定GPU"></a>指定GPU</h3><p>当服务器中有多个GPU的时候，选择指定的GPU运行程序可在程序运行命令之前使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以多个0，1，2</span></span><br></pre></td></tr></table></figure>
<p>也可以将这行指令临时添加到终端反复使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>永久设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在~/.bashrc 的最后加上export CUDA_VISIBLE_DEVICES=<span class="number">1</span>，然后source ~/.bashrc</span><br></pre></td></tr></table></figure>

<h3 id="在代码中指定GPU"><a href="#在代码中指定GPU" class="headerlink" title="在代码中指定GPU"></a>在代码中指定GPU</h3><blockquote>
<p>指定GPU的命令需要放在和神经网络相关的一系列操作之前。</p>
</blockquote>
<h4 id="默认使用某块GPU"><a href="#默认使用某块GPU" class="headerlink" title="默认使用某块GPU"></a>默认使用某块GPU</h4><h5 id="一块"><a href="#一块" class="headerlink" title="一块"></a>一块</h5><p>设置当前使用的GPU设备仅为0号设备，设备名称为 &#x2F;gpu:0：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;0&quot;</span></span><br></pre></td></tr></table></figure>

<h5 id="多块"><a href="#多块" class="headerlink" title="多块"></a>多块</h5><p>设置当前使用的GPU设备为0,1号两个设备，名称依次为 &#x2F;gpu:0、&#x2F;gpu:1：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;0,1&quot;</span></span><br></pre></td></tr></table></figure>
<p>根据顺序表示优先使用0号设备,然后使用1号设备。</p>
<h2 id="查看模型每层输出详情"><a href="#查看模型每层输出详情" class="headerlink" title="查看模型每层输出详情"></a>查看模型每层输出详情</h2><p>Keras有一个简洁的API来查看模型的每一层输出尺寸，这在调试网络时非常有用。现在在PyTorch中也可以实现这个功能。<br>使用很简单，如下用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="comment">#input_size 是根据你自己的网络模型的输入尺寸进行设置。</span></span><br><span class="line">summary(your_model, input_size=(channels, H, W))</span><br></pre></td></tr></table></figure>

<h2 id="梯度裁剪（Gradient-Clipping）"><a href="#梯度裁剪（Gradient-Clipping）" class="headerlink" title="梯度裁剪（Gradient Clipping）"></a>梯度裁剪（Gradient Clipping）</h2><p>梯度裁剪在某些任务上会额外消耗大量的计算时间。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">outputs = model(data)</span><br><span class="line">loss= loss_fn(outputs, target)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">20</span>, norm_type=<span class="number">2</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>nn.utils.clip_grad_norm_的参数：<br><strong>parameters</strong> – 一个基于变量的迭代器，会进行梯度归一化<br><strong>max_norm</strong> – 梯度的最大范数<br><strong>norm_type</strong> – 规定范数的类型，默认为L2</p>
<h2 id="防止验证模型时爆显存"><a href="#防止验证模型时爆显存" class="headerlink" title="防止验证模型时爆显存"></a>防止验证模型时爆显存</h2><p>验证模型时不需要求导，即不需要梯度计算，关闭autograd，可以提高速度，节约内存。如果不关闭可能会爆显存。</p>
<h3 id="no-grad-1"><a href="#no-grad-1" class="headerlink" title="no_grad"></a>no_grad</h3><p>一般来说，我们在进行模型训练的过程中，因为要监控模型的性能，在跑完若干个epoch训练之后，需要进行一次在验证集[4]上的性能验证。一般来说，在验证或者是测试阶段，因为只是需要跑个前向传播(forward)就足够了，<strong>因此不需要保存变量的梯度</strong>。<br><strong>问题：</strong>保存梯度是需要额外显存或者内存进行保存的，占用了空间，有时候还会在验证阶段导致OOM(Out Of Memory)错误，因此我们在验证和测试阶段，最好显式地取消掉模型变量的梯度。通过no_grad实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># here train the model, just skip the codes</span></span><br><span class="line">model.<span class="built_in">eval</span>() <span class="comment"># here we start to evaluate the model</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> eval_data:</span><br><span class="line">data, label = each</span><br><span class="line">logit = model(data)</span><br><span class="line"><span class="meta">... </span><span class="comment"># here we just skip the codes</span></span><br></pre></td></tr></table></figure>
<p>在pytorch0.4之前的版本通过volatile&#x3D;True来进行设置。</p>
<h3 id="empty-cache"><a href="#empty-cache" class="headerlink" title="empty_cache"></a>empty_cache</h3><p>PyTorch的缓存分配器会事先分配一些固定的显存，即使实际上tensors并没有使用完这些显存，这些显存也不能被其他应用使用。这个分配过程由第一次CUDA内存访问触发的。<br>而 torch.cuda.empty_cache() 的作用就是释放缓存分配器当前持有的且未占用的缓存显存，以便这些显存可以被其他GPU应用程序中使用，并且通过 nvidia-smi命令可见。注意使用此命令不会释放tensors占用的显存。</p>
<h2 id="显存不够用"><a href="#显存不够用" class="headerlink" title="显存不够用"></a>显存不够用</h2><h3 id="retain-graph-1"><a href="#retain-graph-1" class="headerlink" title="retain_graph"></a>retain_graph</h3><blockquote>
<p>进行梯度累积，实现内存紧张情况下的大batch_size训练,在GPU显存紧张的情况下使用可以等价于用更大的batch_size进行训练。</p>
</blockquote>
<p>首先我们要明白，当调用.backward()时，其实是对损失到各个节点的梯度进行计算，计算结果将会保存在各个节点上，如果不用opt.zero_grad()对其进行清0，那么只要你一直调用.backward()梯度就会一直累积，没有调用优化器就不会使用这些梯度来更新参数，多次backward之后再调用优化器相当于是在大的batch_size下进行的训练。</p>
<h4 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">net</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="built_in">super</span>().__init__()</span><br><span class="line">self.fc1 = nn.Linear(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">self.act = nn.ReLU()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,inputv</span>):</span><br><span class="line"><span class="keyword">return</span> self.act(self.fc1(inputv))</span><br><span class="line">n = net()</span><br><span class="line">inputv = torch.tensor(np.random.normal(size=(<span class="number">4</span>,<span class="number">10</span>))).<span class="built_in">float</span>()</span><br><span class="line">output = n(inputv)</span><br><span class="line">target = torch.tensor(np.ones((<span class="number">4</span>,<span class="number">2</span>))).<span class="built_in">float</span>()</span><br><span class="line">loss = nn.functional.mse_loss(output, target)</span><br><span class="line">loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">opt = torch.optim.Adam(n.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> n.parameters():</span><br><span class="line"><span class="built_in">print</span>(each.grad)</span><br></pre></td></tr></table></figure>
<p>第一次输出:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0.0493</span>, -<span class="number">0.0581</span>, -<span class="number">0.0451</span>,  <span class="number">0.0485</span>,  <span class="number">0.1147</span>,  <span class="number">0.1413</span>, -<span class="number">0.0712</span>, -<span class="number">0.1459</span>,</span><br><span class="line"><span class="number">0.1090</span>, -<span class="number">0.0896</span>],</span><br><span class="line">[ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line"><span class="number">0.0000</span>,  <span class="number">0.0000</span>]])</span><br><span class="line">tensor([-<span class="number">0.1192</span>,  <span class="number">0.0000</span>])</span><br></pre></td></tr></table></figure>
<p>第二次loss.backward(retain_graph&#x3D;True)，输出为:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0.0987</span>, -<span class="number">0.1163</span>, -<span class="number">0.0902</span>,  <span class="number">0.0969</span>,  <span class="number">0.2295</span>,  <span class="number">0.2825</span>, -<span class="number">0.1424</span>, -<span class="number">0.2917</span>,</span><br><span class="line"><span class="number">0.2180</span>, -<span class="number">0.1792</span>],</span><br><span class="line">[ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line"><span class="number">0.0000</span>,  <span class="number">0.0000</span>]])</span><br><span class="line">tensor([-<span class="number">0.2383</span>,  <span class="number">0.0000</span>])</span><br></pre></td></tr></table></figure>
<p>运行一次opt.zero_grad()，输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>现在明白为什么我们一般在求梯度时要用opt.zero_grad()了吧，那是为了不要这次的梯度结果被上一次给影响，但是在某些情况下这个‘影响’是可以利用的。</p>
</blockquote>
<h2 id="扩充Batch"><a href="#扩充Batch" class="headerlink" title="扩充Batch"></a>扩充Batch</h2><blockquote>
<p>可以通过滞后zero_grad实现batch扩大</p>
</blockquote>
<h1 id="问题汇总"><a href="#问题汇总" class="headerlink" title="问题汇总"></a>问题汇总</h1><h2 id="Expected-object-of-device-type-cuda-but-got-device-type-cpu"><a href="#Expected-object-of-device-type-cuda-but-got-device-type-cpu" class="headerlink" title="Expected object of device type cuda but got device type cpu"></a>Expected object of device type cuda but got device type cpu</h2><p>这种问题一般是由于to(device)使用不规范导致的。同时to(device)操作也容易导致性能下降。这里建议在模型已经在cuda上的时候，使用like的操作来生成需要的张量数据:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros_like()</span><br><span class="line">torch.ones_like()</span><br><span class="line">torch.rand_like()</span><br><span class="line">torch.randn_like()</span><br><span class="line">torch.randint_like()</span><br><span class="line">torch.empty_like()</span><br><span class="line">torch.full_like()</span><br></pre></td></tr></table></figure>

<h2 id="AssertionError-nn-criterions-don’t-compute-the-gradient-w-r-t-targets-please-mark-these-variables-as-volatile-or-not-requiring-gradients"><a href="#AssertionError-nn-criterions-don’t-compute-the-gradient-w-r-t-targets-please-mark-these-variables-as-volatile-or-not-requiring-gradients" class="headerlink" title="AssertionError: nn criterions don’t compute the gradient w.r.t. targets please mark these variables as volatile or not requiring gradients"></a>AssertionError: nn criterions don’t compute the gradient w.r.t. targets please mark these variables as volatile or not requiring gradients</h2><p>表示target是需要一个不能被训练的，也就是requires_grad&#x3D;False的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">target = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">output = loss(<span class="built_in">input</span>, target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>MSELoss，和其他很多loss，比如交叉熵，KL散度等，其target都需要是一个不能被训练的值的，这个和TensorFlow中的tf.nn.softmax_cross_entropy_with_logits_v2不太一样，后者可以使用可训练的target。</p>
</blockquote>
<h1 id="必知必会"><a href="#必知必会" class="headerlink" title="必知必会"></a>必知必会</h1><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>合适的初始化很重要，初始化就跟黑科技一样，用对了超参都不用调；没用对，跑出来的结果就跟模型有bug一样不忍直视。<br>优选xavier初始化或者He初始化。</p>
<h2 id="1xN卷积"><a href="#1xN卷积" class="headerlink" title="1xN卷积"></a>1xN卷积</h2><p>适当使用，可能得到更好的泛化效果：</p>
<ol>
<li>卷积可以减少计算量</li>
<li>可以在某个方向强调感受野，也就是说假如如果你要对一个长方形形状的目标进行分类，你可以使用的卷积核搭配的卷积核对长边方向设定更大的感受野</li>
</ol>
<h2 id="ACNet结构"><a href="#ACNet结构" class="headerlink" title="ACNet结构"></a>ACNet结构</h2><p>在3x3卷积的基础上加上1x3和3x1的旁路卷积核，最后在推理阶段把三个卷积核都fusion到3x3卷积核上，在许多经典CV任务上都可以获得大概1个点的提升。</p>
<h2 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h2><p>加速收敛。</p>
<blockquote>
<p>如果有BN了全连接层就没必要加Dropout了。</p>
</blockquote>
<h2 id="fpn结构"><a href="#fpn结构" class="headerlink" title="fpn结构"></a>fpn结构</h2><p>目标检测<strong>不能盲目</strong>去掉fpn结构。在针对自己的数据调检测任务如yolov3的时候不能盲目砍掉fpn结构，尽管你分析出某个分支的Anchor基本不可能会对你预测的目标起作用，但如果你直接去掉分支很可能会带来漏检。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>可以先用ReLU做一版，如果想再提升精度可以将ReLU改成PReLU试试。我更倾向于直接使用ReLU。</p>
<h2 id="batch-size"><a href="#batch-size" class="headerlink" title="batch_size"></a>batch_size</h2><p>在不同类型的任务中，batch_size的影响也不同。</p>
<h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><p>用loss的时候往往并不是直接替换loss那么简单，需要仔细思考loss背后的数学原理，要用对地方才可有提升。<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/293369755">为什么 YOLOv3 用了 Focal Loss 后 mAP 反而掉了？</a></p>
<h2 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h2><p>使用了带backbone的网络，如训练VGG16-SSD建议选择finetune的方式，从头训练不仅费时费力，甚至难以收敛。</p>
<h2 id="upsampling"><a href="#upsampling" class="headerlink" title="upsampling"></a>upsampling</h2><p>在做分割实验的时候我发现用upsamling 加1*1卷积代替反卷积做上采样得到的结果更平滑，并且miou差距不大，所以我认为这两者都是都可以使用的。</p>
<h2 id="框过多也是需要优化的"><a href="#框过多也是需要优化的" class="headerlink" title="框过多也是需要优化的"></a>框过多也是需要优化的</h2><p>一些Anchor-based目标检测算法为了提高精度，都是疯狂给框，ap值确实上去了，但也导致了fp会很多，并且这部分fp没有回归，在nms阶段也滤不掉。相比于ap提升而言，工程上减少fp更加重要。Gaussian yolov3的fp相比于yolov3会减少40％</p>
<h2 id="epoch-or-iteration"><a href="#epoch-or-iteration" class="headerlink" title="epoch or iteration"></a>epoch or iteration</h2><h3 id="what-1"><a href="#what-1" class="headerlink" title="what"></a>what</h3><p>现在主要有4种迭代参数的方式：</p>
<ol>
<li>Loss&#x2F;Epoch：告诉你一个模型要观察同一张图片多少次才能理解它(遍历所有数据才进行更新)</li>
<li>Loss&#x2F;Iteration：告诉你需要多少次参数更新。(由于每次Batch的采样已经是独立同分布)(比较优化器时这很有用，可以帮助你加快训练速度或达到更高的精度)</li>
<li>Loss&#x2F;Total Image Seen(iteration * batchsize)：告诉你算法看到了多少图像时的损失。适合比较两种算法使用数据的效率。并能够消除Batchsize的影响，这允许在不同GPU上训练的具有不同Batch Size的模型之间进行公平地比较。</li>
<li>Loss&#x2F;Time(on specific dedicated gpu(s))</li>
</ol>
<h2 id="seed"><a href="#seed" class="headerlink" title="seed"></a>seed</h2><p>需要为Pytorch和numpy分别设置各自的随机生成数。<br>比如在进行DataLoader的时候，使用worker_init_fn选项专门设置 seed，否则在 PyTorch 同时使用 NumPy 的随机数生成器和多进程数据加载会导致相同的扩充数据。用户没有这样做，因而这个 bug 悄悄地降低了模型的准确率。<br><strong>如果不针对生成则会使用同一个生成数从而降低模型的准确率。</strong></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1684007">池化</a><br><a target="_blank" rel="noopener" href="https://www.sohu.com/a/442710521_823210">池化2</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103342289">平移</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38024868">2</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34169502">自定义op</a><br><a target="_blank" rel="noopener" href="https://bbs.csdn.net/topics/390798229?utm_medium=distribute.pc_relevant.none-task-discussion_topic-BlogCommendFromBaidu-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-discussion_topic-BlogCommendFromBaidu-1.control">2</a><br><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/gNFfBXktyPBJAjJHgGCJkw">反向传播</a><br><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247525289&idx=3&sn=2c42a67f4ec09620b717ced2c0e2e3c9&chksm=ec1c8e50db6b0746ed36cefe03e85477bf7c1511c9cddd5d5789b689fd755a387d918bff0467&mpshare=1&scene=24&srcid=1208mmcGeXYiZoC6X77ycvPx&sharer_sharetime=1607436292561&sharer_shareid=8d17b4e80d81af2331e5d78c79a59f63#rd">SBN</a></p>
<h1 id="推荐库"><a href="#推荐库" class="headerlink" title="推荐库"></a>推荐库</h1><h2 id="kornia"><a href="#kornia" class="headerlink" title="kornia"></a>kornia</h2><p>可以进行Tesnor操作的现代化opencv(针对numpy)。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">ZCDu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/04/15/pytorch/">http://example.com/2022/04/15/pytorch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">赵某人の杂物室</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://img2.baidu.com/it/u=3234735447,1138674362&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=500&amp;h=471" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/04/17/Linux%E6%8F%90%E6%9D%83/"><img class="prev-cover" src="https://img0.baidu.com/it/u=1563243320,787833354&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=440&amp;h=440" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Linux提权</div></div></a></div><div class="next-post pull-right"><a href="/2022/04/14/TokenPose/"><img class="next-cover" src="https://img2.baidu.com/it/u=633687970,3181534032&amp;fm=253&amp;fmt=auto&amp;app=120&amp;f=JPEG?w=700&amp;h=653" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">TokenPose</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC81NTk2Ni8zMjQyOQ=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://hexo-logseq.oss-cn-hangzhou.aliyuncs.com/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ZCDu</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">36</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ZCDu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ZCDu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://gitee.com/zcdu" target="_blank" title="Gitee"><i class="fab fa-gitlab"></i></a><a class="social-icon" href="mailto:437677825@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">好想毕业！！！！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#ConvTranspose2d"><span class="toc-number">1.1.</span> <span class="toc-text">ConvTranspose2d</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nn-AdaptiveAvgPool2d"><span class="toc-number">1.2.</span> <span class="toc-text">nn.AdaptiveAvgPool2d</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%85%83-2d"><span class="toc-number">1.2.1.</span> <span class="toc-text">2元(2d)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%87%E8%81%9A%E5%B1%82%EF%BC%88Pooling%EF%BC%89"><span class="toc-number">1.2.2.</span> <span class="toc-text">汇聚层（Pooling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E5%80%BC%EF%BC%88Avg%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">均值（Avg）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E9%80%82%E5%BA%94-Adaptive"><span class="toc-number">1.2.4.</span> <span class="toc-text">自适应(Adaptive)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-AvgPool2d"><span class="toc-number">1.2.5.</span> <span class="toc-text">nn.AvgPool2d</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedding"><span class="toc-number">1.3.</span> <span class="toc-text">Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.1.</span> <span class="toc-text">函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8"><span class="toc-number">1.3.2.</span> <span class="toc-text">使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">传统的使用方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8Sparse-R-CNN%E4%B8%AD%E7%94%A8%E6%9D%A5%E5%B5%8C%E5%85%A5%E6%A1%86"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">在Sparse R-CNN中用来嵌入框</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95-1"><span class="toc-number">1.4.</span> <span class="toc-text">传统的使用方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#detach"><span class="toc-number">1.5.</span> <span class="toc-text">detach</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NMS"><span class="toc-number">1.6.</span> <span class="toc-text">NMS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">1.6.1.</span> <span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%A7%8Dnms"><span class="toc-number">1.6.2.</span> <span class="toc-text">多种nms</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hard-nms"><span class="toc-number">1.6.2.1.</span> <span class="toc-text">Hard-nms</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Soft-nms"><span class="toc-number">1.6.2.2.</span> <span class="toc-text">Soft-nms</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#and-nms"><span class="toc-number">1.6.2.3.</span> <span class="toc-text">and-nms</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#merge-nms"><span class="toc-number">1.6.2.4.</span> <span class="toc-text">merge-nms</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#diou-nms"><span class="toc-number">1.6.2.5.</span> <span class="toc-text">diou-nms</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CODE"><span class="toc-number">1.6.3.</span> <span class="toc-text">CODE</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#NMS-python%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.6.3.1.</span> <span class="toc-text">NMS python实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#IOU%E8%AE%A1%E7%AE%97"><span class="toc-number">1.6.3.2.</span> <span class="toc-text">IOU计算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%8C%E8%A7%84%E8%8C%83%E5%8C%96"><span class="toc-number">1.7.</span> <span class="toc-text">正则化，规范化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#standardization"><span class="toc-number">1.7.1.</span> <span class="toc-text">standardization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normalization"><span class="toc-number">1.7.2.</span> <span class="toc-text">Normalization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96"><span class="toc-number">1.8.</span> <span class="toc-text">池化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="toc-number">1.9.</span> <span class="toc-text">平移不变性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89OP"><span class="toc-number">1.10.</span> <span class="toc-text">自定义OP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%8F%82%E6%95%B0%E9%87%8F"><span class="toc-number">1.11.</span> <span class="toc-text">统计参数量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.12.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%B4%E5%BA%A6%E5%88%86%E6%9E%90"><span class="toc-number">1.12.1.</span> <span class="toc-text">维度分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.12.1.1.</span> <span class="toc-text">示例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-number">1.12.1.1.1.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B1%82%E8%A7%A3"><span class="toc-number">1.12.1.1.2.</span> <span class="toc-text">求解</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%9D%E8%B7%AF"><span class="toc-number">1.12.1.1.3.</span> <span class="toc-text">思路</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-number">1.12.2.</span> <span class="toc-text">链式法则</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BN%E9%9B%86%E5%90%88"><span class="toc-number">1.13.</span> <span class="toc-text">BN集合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#traditional-BN"><span class="toc-number">1.13.1.</span> <span class="toc-text">traditional BN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%AF%B9%E8%B1%A1%E7%9A%84%E9%AD%94%E6%94%B9"><span class="toc-number">1.13.2.</span> <span class="toc-text">统计对象的魔改</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Layer-Normization"><span class="toc-number">1.13.2.1.</span> <span class="toc-text">Layer Normization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Instance-Normization"><span class="toc-number">1.13.2.2.</span> <span class="toc-text">Instance Normization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Group-Normization"><span class="toc-number">1.13.2.3.</span> <span class="toc-text">Group Normization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Switchable-Normalization"><span class="toc-number">1.13.2.4.</span> <span class="toc-text">Switchable Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Sync-Batch-Normalization"><span class="toc-number">1.13.2.5.</span> <span class="toc-text">Sync Batch Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.13.2.6.</span> <span class="toc-text">实现</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#2%E6%AC%A1%E5%90%8C%E6%AD%A5"><span class="toc-number">1.13.2.6.0.1.</span> <span class="toc-text">2次同步</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%91%E6%AC%A1%E5%90%8C%E6%AD%A5"><span class="toc-number">1.13.2.6.0.2.</span> <span class="toc-text">１次同步</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Sparse-Switchable-Normalization"><span class="toc-number">1.13.2.7.</span> <span class="toc-text">Sparse Switchable Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Region-Normalization"><span class="toc-number">1.13.2.8.</span> <span class="toc-text">Region Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Local-Context-Normalization"><span class="toc-number">1.13.2.9.</span> <span class="toc-text">Local Context Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Channel-Normalization"><span class="toc-number">1.13.2.10.</span> <span class="toc-text">Channel Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Domain-Specific-Batch-Normalization"><span class="toc-number">1.13.2.11.</span> <span class="toc-text">Domain-Specific Batch Normalization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%84%E8%8C%83%E5%8C%96%E6%96%B9%E5%BC%8F%E7%9A%84%E9%AD%94%E6%94%B9"><span class="toc-number">1.13.3.</span> <span class="toc-text">规范化方式的魔改</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Filter-Response-Normalization"><span class="toc-number">1.13.3.1.</span> <span class="toc-text">Filter Response Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Extended-Batch-Normalization"><span class="toc-number">1.13.3.2.</span> <span class="toc-text">Extended Batch Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kalman-Normalization"><span class="toc-number">1.13.3.3.</span> <span class="toc-text">Kalman Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#L1-Norm-Batch-Normalization"><span class="toc-number">1.13.3.4.</span> <span class="toc-text">L1-Norm Batch Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Cosine-Normalization"><span class="toc-number">1.13.3.5.</span> <span class="toc-text">Cosine Normalization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E6%96%B9%E5%BC%8F%E7%9A%84%E9%AD%94%E6%94%B9"><span class="toc-number">1.13.4.</span> <span class="toc-text">仿射变换方式的魔改</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Conditional-Batch-x2F-Instance-Normalization"><span class="toc-number">1.13.4.1.</span> <span class="toc-text">Conditional Batch&#x2F;Instance Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adaptive-Instance-Normalization"><span class="toc-number">1.13.4.2.</span> <span class="toc-text">Adaptive Instance Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Adaptive-Convolution-based-Normalization"><span class="toc-number">1.13.4.3.</span> <span class="toc-text">Adaptive Convolution-based Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spatially-Adaptive-Normalization"><span class="toc-number">1.13.4.4.</span> <span class="toc-text">Spatially-Adaptive Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Region-Adaptive-Normalization"><span class="toc-number">1.13.4.5.</span> <span class="toc-text">Region-Adaptive Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Instance-Enhancement-Batch-Normalization"><span class="toc-number">1.13.4.6.</span> <span class="toc-text">Instance Enhancement Batch Normalization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Attentive-Normalization"><span class="toc-number">1.13.4.7.</span> <span class="toc-text">Attentive Normalization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.13.5.</span> <span class="toc-text"></span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.14.</span> <span class="toc-text">保存加载模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95"><span class="toc-number">1.14.1.</span> <span class="toc-text">保存加载模型基本用法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.14.1.1.</span> <span class="toc-text">整个模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AA%E9%92%88%E5%AF%B9%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">1.14.1.2.</span> <span class="toc-text">只针对模型参数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0"><span class="toc-number">1.14.2.</span> <span class="toc-text">加载部分参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pkl%E6%96%87%E4%BB%B6"><span class="toc-number">1.14.3.</span> <span class="toc-text">pkl文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.14.4.</span> <span class="toc-text">自定义模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98"><span class="toc-number">1.14.4.1.</span> <span class="toc-text">保存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.14.4.2.</span> <span class="toc-text">加载</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#state-dict"><span class="toc-number">1.14.5.</span> <span class="toc-text">state_dict</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">1.14.5.1.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%A8%E8%AE%BE%E5%A4%87%E4%BF%9D%E5%AD%98%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.14.6.</span> <span class="toc-text">跨设备保存加载模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Save-on-GPU-Load-on-CPU"><span class="toc-number">1.14.6.1.</span> <span class="toc-text">Save on GPU, Load on CPU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Save-on-GPU-Load-on-GPU"><span class="toc-number">1.14.6.2.</span> <span class="toc-text">Save on GPU, Load on GPU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Save-on-CPU-Load-on-GPU"><span class="toc-number">1.14.6.3.</span> <span class="toc-text">Save on CPU, Load on GPU</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CUDA"><span class="toc-number">1.14.7.</span> <span class="toc-text">CUDA</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A4%E6%96%ADcuda%E6%98%AF%E5%90%A6%E5%8F%AF%E7%94%A8"><span class="toc-number">1.14.7.1.</span> <span class="toc-text">判断cuda是否可用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96gpu%E6%95%B0%E9%87%8F"><span class="toc-number">1.14.7.2.</span> <span class="toc-text">获取gpu数量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96gpu%E5%90%8D%E5%AD%97"><span class="toc-number">1.14.7.3.</span> <span class="toc-text">获取gpu名字</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E%E5%BD%93%E5%89%8Dgpu%E8%AE%BE%E5%A4%87%E7%B4%A2%E5%BC%95%EF%BC%8C%E9%BB%98%E8%AE%A4%E4%BB%8E0%E5%BC%80%E5%A7%8B"><span class="toc-number">1.14.7.4.</span> <span class="toc-text">返回当前gpu设备索引，默认从0开始</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8Btensor%E6%88%96%E8%80%85model%E5%9C%A8%E5%93%AA%E5%9D%97GPU%E4%B8%8A"><span class="toc-number">1.14.7.5.</span> <span class="toc-text">查看tensor或者model在哪块GPU上</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E6%95%B0%E6%8D%AE%E5%92%8C%E6%A8%A1%E5%9E%8B%E4%BB%8Ecpu%E8%BF%81%E7%A7%BB%E5%88%B0gpu%E4%B8%8A"><span class="toc-number">1.14.7.6.</span> <span class="toc-text">将数据和模型从cpu迁移到gpu上</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#no-grad"><span class="toc-number">1.15.</span> <span class="toc-text">no_grad</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#model-eval"><span class="toc-number">1.16.</span> <span class="toc-text">model.eval</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#retain-graph"><span class="toc-number">1.17.</span> <span class="toc-text">retain_graph</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E9%87%8A%E6%94%BE"><span class="toc-number">1.17.1.</span> <span class="toc-text">为什么不释放</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-2"><span class="toc-number">1.17.1.1.</span> <span class="toc-text">示例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%BB%E7%BB%93%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="toc-number">1.18.</span> <span class="toc-text">冻结网络层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#eval"><span class="toc-number">1.18.1.</span> <span class="toc-text">eval</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%BB%E7%BB%93%E6%8C%87%E5%AE%9A%E7%9A%84%E5%B1%82"><span class="toc-number">1.18.2.</span> <span class="toc-text">冻结指定的层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#named-parameters%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.18.3.</span> <span class="toc-text">named_parameters实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AA%E5%86%BB%E7%BB%93BN"><span class="toc-number">1.18.4.</span> <span class="toc-text">只冻结BN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98-1"><span class="toc-number">1.18.4.1.</span> <span class="toc-text">问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#solution"><span class="toc-number">1.18.4.2.</span> <span class="toc-text">solution</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#class%E5%B1%9E%E6%80%A7"><span class="toc-number">1.18.4.2.1.</span> <span class="toc-text">class属性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#apply"><span class="toc-number">1.18.4.2.2.</span> <span class="toc-text">apply</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%80%E7%BB%88%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.18.4.2.3.</span> <span class="toc-text">最终实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%BA%8C"><span class="toc-number">1.18.4.3.</span> <span class="toc-text">实现二</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#dataloader"><span class="toc-number">2.1.</span> <span class="toc-text">dataloader</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sampler"><span class="toc-number">2.2.</span> <span class="toc-text">Sampler</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">2.2.1.</span> <span class="toc-text">实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F"><span class="toc-number">2.2.2.</span> <span class="toc-text">注意</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSet"><span class="toc-number">2.3.</span> <span class="toc-text">DataSet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E5%B1%95"><span class="toc-number">2.3.1.</span> <span class="toc-text">扩展</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor"><span class="toc-number">2.4.</span> <span class="toc-text">Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Variable"><span class="toc-number">2.4.1.</span> <span class="toc-text">Variable</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameter"><span class="toc-number">2.4.2.</span> <span class="toc-text">Parameter</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">2.4.2.1.</span> <span class="toc-text">实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A6%81%E7%82%B9"><span class="toc-number">2.4.2.2.</span> <span class="toc-text">要点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#named-parameters"><span class="toc-number">2.4.3.</span> <span class="toc-text">named_parameters()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#buffer"><span class="toc-number">2.4.4.</span> <span class="toc-text">buffer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E5%B9%B6%E8%BF%90%E7%AE%97"><span class="toc-number">2.4.5.</span> <span class="toc-text">归并运算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataParallel"><span class="toc-number">2.5.</span> <span class="toc-text">DataParallel</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E7%A4%BA%E4%BE%8B"><span class="toc-number">2.5.1.</span> <span class="toc-text">基础示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">2.5.2.</span> <span class="toc-text">缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E6%9E%90"><span class="toc-number">2.5.2.1.</span> <span class="toc-text">分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataParallel-CODE"><span class="toc-number">2.5.3.</span> <span class="toc-text">DataParallel CODE</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">高阶函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#gather"><span class="toc-number">3.1.</span> <span class="toc-text">gather</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#collate-fn"><span class="toc-number">3.2.</span> <span class="toc-text">collate_fn</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%BC%E6%8E%A5"><span class="toc-number">3.2.1.</span> <span class="toc-text">拼接</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2"><span class="toc-number">3.3.</span> <span class="toc-text">维度变换</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#view"><span class="toc-number">4.</span> <span class="toc-text">view</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#numpy-newaxis"><span class="toc-number">5.</span> <span class="toc-text">numpy.newaxis</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#unsqueeze"><span class="toc-number">6.</span> <span class="toc-text">unsqueeze</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvision-models"><span class="toc-number">6.1.</span> <span class="toc-text">torchvision.models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E9%83%A8%E5%88%86%E6%A0%B7%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">6.2.</span> <span class="toc-text">查看部分样本信息</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-index-select"><span class="toc-number">6.2.1.</span> <span class="toc-text">torch.index_select()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvision"><span class="toc-number">6.3.</span> <span class="toc-text">torchvision</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ops"><span class="toc-number">6.3.1.</span> <span class="toc-text">ops</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ray"><span class="toc-number">6.4.</span> <span class="toc-text">ray</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95"><span class="toc-number">6.5.</span> <span class="toc-text">索引</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-index-select-input-dim-index"><span class="toc-number">6.5.1.</span> <span class="toc-text">torch.index_select(input,dim,index)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-masked-select-input-mask"><span class="toc-number">6.5.2.</span> <span class="toc-text">torch.masked_select(input,mask)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Function"><span class="toc-number">6.6.</span> <span class="toc-text">Function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ctx"><span class="toc-number">6.6.1.</span> <span class="toc-text">ctx</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A5ChannNorm%E4%B8%BA%E4%BE%8B"><span class="toc-number">6.6.2.</span> <span class="toc-text">以ChannNorm为例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hook"><span class="toc-number">6.7.</span> <span class="toc-text">hook</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#why"><span class="toc-number">6.7.1.</span> <span class="toc-text">why</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hook-for-Tensors"><span class="toc-number">6.7.2.</span> <span class="toc-text">Hook for Tensors</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%98%E5%9C%A8%E6%84%8F%E4%B9%89"><span class="toc-number">6.7.2.1.</span> <span class="toc-text">存在意义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8"><span class="toc-number">6.7.2.2.</span> <span class="toc-text">应用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9%E4%B8%AD%E9%97%B4%E7%BB%93%E6%9E%9C%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="toc-number">6.7.2.2.1.</span> <span class="toc-text">修改中间结果的梯度</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E4%B8%AD%E9%97%B4%E8%8A%82%E7%82%B9%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="toc-number">6.7.2.2.2.</span> <span class="toc-text">查看中间节点的梯度</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hook-for-Modules"><span class="toc-number">6.7.3.</span> <span class="toc-text">Hook for Modules</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#why-1"><span class="toc-number">6.7.3.1.</span> <span class="toc-text">why</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how"><span class="toc-number">6.8.</span> <span class="toc-text">how</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#what"><span class="toc-number">6.8.0.1.</span> <span class="toc-text">what</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-nn-Module-register-forward-hook%E2%86%92None"><span class="toc-number">6.8.0.1.1.</span> <span class="toc-text">torch.nn.Module.register_forward_hook→None</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-nn-Module-register-forward-pre-hook"><span class="toc-number">6.8.0.1.2.</span> <span class="toc-text">torch.nn.Module.register_forward_pre_hook</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#torch-nn-Module-register-backward-hook%E2%86%92Tensor-or-None"><span class="toc-number">6.8.0.1.3.</span> <span class="toc-text">torch.nn.Module.register_backward_hook→Tensor or None</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8A%80%E5%B7%A7"><span class="toc-number">7.</span> <span class="toc-text">技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E8%AF%95"><span class="toc-number">7.1.</span> <span class="toc-text">调试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%88%E7%AB%AF%E6%9D%A1%E4%BB%B6"><span class="toc-number">7.1.1.</span> <span class="toc-text">终端条件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E5%87%BD%E6%95%B0%E5%8A%9F%E8%83%BD"><span class="toc-number">7.1.2.</span> <span class="toc-text">查看函数功能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9AGPU"><span class="toc-number">7.1.3.</span> <span class="toc-text">指定GPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E4%BB%A3%E7%A0%81%E4%B8%AD%E6%8C%87%E5%AE%9AGPU"><span class="toc-number">7.1.4.</span> <span class="toc-text">在代码中指定GPU</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%E4%BD%BF%E7%94%A8%E6%9F%90%E5%9D%97GPU"><span class="toc-number">7.1.4.1.</span> <span class="toc-text">默认使用某块GPU</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%80%E5%9D%97"><span class="toc-number">7.1.4.1.1.</span> <span class="toc-text">一块</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%9D%97"><span class="toc-number">7.1.4.1.2.</span> <span class="toc-text">多块</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%A8%A1%E5%9E%8B%E6%AF%8F%E5%B1%82%E8%BE%93%E5%87%BA%E8%AF%A6%E6%83%85"><span class="toc-number">7.2.</span> <span class="toc-text">查看模型每层输出详情</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%EF%BC%88Gradient-Clipping%EF%BC%89"><span class="toc-number">7.3.</span> <span class="toc-text">梯度裁剪（Gradient Clipping）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%98%B2%E6%AD%A2%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B%E6%97%B6%E7%88%86%E6%98%BE%E5%AD%98"><span class="toc-number">7.4.</span> <span class="toc-text">防止验证模型时爆显存</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#no-grad-1"><span class="toc-number">7.4.1.</span> <span class="toc-text">no_grad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#empty-cache"><span class="toc-number">7.4.2.</span> <span class="toc-text">empty_cache</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%BE%E5%AD%98%E4%B8%8D%E5%A4%9F%E7%94%A8"><span class="toc-number">7.5.</span> <span class="toc-text">显存不够用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#retain-graph-1"><span class="toc-number">7.5.1.</span> <span class="toc-text">retain_graph</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-3"><span class="toc-number">7.5.1.1.</span> <span class="toc-text">示例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A9%E5%85%85Batch"><span class="toc-number">7.6.</span> <span class="toc-text">扩充Batch</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB"><span class="toc-number">8.</span> <span class="toc-text">问题汇总</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Expected-object-of-device-type-cuda-but-got-device-type-cpu"><span class="toc-number">8.1.</span> <span class="toc-text">Expected object of device type cuda but got device type cpu</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AssertionError-nn-criterions-don%E2%80%99t-compute-the-gradient-w-r-t-targets-please-mark-these-variables-as-volatile-or-not-requiring-gradients"><span class="toc-number">8.2.</span> <span class="toc-text">AssertionError: nn criterions don’t compute the gradient w.r.t. targets please mark these variables as volatile or not requiring gradients</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A"><span class="toc-number">9.</span> <span class="toc-text">必知必会</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">9.1.</span> <span class="toc-text">初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1xN%E5%8D%B7%E7%A7%AF"><span class="toc-number">9.2.</span> <span class="toc-text">1xN卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ACNet%E7%BB%93%E6%9E%84"><span class="toc-number">9.3.</span> <span class="toc-text">ACNet结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BN"><span class="toc-number">9.4.</span> <span class="toc-text">BN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#fpn%E7%BB%93%E6%9E%84"><span class="toc-number">9.5.</span> <span class="toc-text">fpn结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">9.6.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#batch-size"><span class="toc-number">9.7.</span> <span class="toc-text">batch_size</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loss"><span class="toc-number">9.8.</span> <span class="toc-text">loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#backbone"><span class="toc-number">9.9.</span> <span class="toc-text">backbone</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#upsampling"><span class="toc-number">9.10.</span> <span class="toc-text">upsampling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%86%E8%BF%87%E5%A4%9A%E4%B9%9F%E6%98%AF%E9%9C%80%E8%A6%81%E4%BC%98%E5%8C%96%E7%9A%84"><span class="toc-number">9.11.</span> <span class="toc-text">框过多也是需要优化的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#epoch-or-iteration"><span class="toc-number">9.12.</span> <span class="toc-text">epoch or iteration</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-1"><span class="toc-number">9.12.1.</span> <span class="toc-text">what</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#seed"><span class="toc-number">9.13.</span> <span class="toc-text">seed</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">10.</span> <span class="toc-text">参考</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E5%BA%93"><span class="toc-number">11.</span> <span class="toc-text">推荐库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#kornia"><span class="toc-number">11.1.</span> <span class="toc-text">kornia</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/21/hello-world/" title="Hello World"><img src="https://img2.baidu.com/it/u=3049661996,3985495076&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=640&amp;h=426" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2023/01/21/hello-world/" title="Hello World">Hello World</a><time datetime="2023-01-21T11:59:37.594Z" title="发表于 2023-01-21 11:59:37">2023-01-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/06/zotero/" title="zotero"><img src="https://img2.baidu.com/it/u=3298204190,1532220953&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=440&amp;h=440" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="zotero"/></a><div class="content"><a class="title" href="/2022/05/06/zotero/" title="zotero">zotero</a><time datetime="2022-05-06T18:30:08.000Z" title="发表于 2022-05-06 18:30:08">2022-05-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/30/%E6%96%87%E7%8C%AE%E5%B7%A5%E5%85%B7/" title="文献工具"><img src="https://img2.baidu.com/it/u=633687970,3181534032&amp;fm=253&amp;fmt=auto&amp;app=120&amp;f=JPEG?w=700&amp;h=653" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献工具"/></a><div class="content"><a class="title" href="/2022/04/30/%E6%96%87%E7%8C%AE%E5%B7%A5%E5%85%B7/" title="文献工具">文献工具</a><time datetime="2022-04-30T17:14:31.000Z" title="发表于 2022-04-30 17:14:31">2022-04-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/25/Roam-Research/" title="Roam Research"><img src="https://img2.baidu.com/it/u=3049661996,3985495076&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=640&amp;h=426" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Roam Research"/></a><div class="content"><a class="title" href="/2022/04/25/Roam-Research/" title="Roam Research">Roam Research</a><time datetime="2022-04-25T18:46:10.000Z" title="发表于 2022-04-25 18:46:10">2022-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/19/zerotier/" title="zerotier"><img src="https://img0.baidu.com/it/u=578397616,2670128678&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=440&amp;h=440" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="zerotier"/></a><div class="content"><a class="title" href="/2022/04/19/zerotier/" title="zerotier">zerotier</a><time datetime="2022-04-19T21:45:56.000Z" title="发表于 2022-04-19 21:45:56">2022-04-19</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By ZCDu</div><div class="footer_custom_text">Weclome to my blog!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Livere' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>